{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multitcn_components import TCNStack, DownsampleLayerWithAttention, LearningRateLogger\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import tensorflow_addons as tfa\n",
    "import uuid\n",
    "import sys\n",
    "from scipy.signal import correlate\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, time_series_number, window_size):\n",
    "    \"\"\"\n",
    "    Returns a windowed dataset from a Pandas dataframe\n",
    "    \"\"\"\n",
    "    available_examples= series.shape[0]-window_size + 1\n",
    "    time_series_number = series.shape[1]\n",
    "    inputs = np.zeros((available_examples,window_size,time_series_number))\n",
    "    for i in range(available_examples):\n",
    "        inputs[i,:,:] = series[i:i+window_size,:]\n",
    "    return inputs \n",
    "\n",
    "def windowed_forecast(series, forecast_horizon):\n",
    "    available_outputs = series.shape[0]- forecast_horizon + 1\n",
    "    output_series_num = series.shape[1]\n",
    "    output = np.zeros((available_outputs,forecast_horizon, output_series_num))\n",
    "    for i in range(available_outputs):\n",
    "        output[i,:]= series[i:i+forecast_horizon,:]\n",
    "    return output\n",
    "\n",
    "def shuffle_arrays_together(a,b):\n",
    "    p = np.random.permutation(a.shape[0])\n",
    "    return a[p],b[p]\n",
    "\n",
    "def norm_cross_corr(a,b):\n",
    "    nom = correlate(a,b)\n",
    "    den = np.sqrt(np.sum(np.power(a,2))*np.sum(np.power(b,2)))\n",
    "    return nom/den\n",
    "\n",
    "def symm_mape(true,prediction):\n",
    "    return 100*np.sum(2*np.abs(prediction-true)/(np.abs(true)+np.abs(prediction)))/true.size\n",
    "\n",
    "def get_metrics(true,prediction,print_metrics=False):\n",
    "        c = norm_cross_corr(true,prediction)\n",
    "        extent = int((c.shape[0]-1)/2)\n",
    "        max_corr_point = np.argmax(c)-extent\n",
    "        max_corr = np.max(c)\n",
    "        max_v = np.max(prediction)\n",
    "        mse = mean_squared_error(true,prediction,squared=True)\n",
    "        rmse = mean_squared_error(true,prediction,squared=False)\n",
    "        mae = mean_absolute_error(true,prediction)\n",
    "        r2 = r2_score(true,prediction)\n",
    "        smape = symm_mape(true,prediction)\n",
    "        if print_metrics:\n",
    "            print(\"Max %f - Autocorr %d - MSE %f - RMSE %f - MAE %f - sMAPE %f%% - R^2 %f\"%(max_v,max_corr_point,mse,rmse,mae,smape,r2))\n",
    "        return [max_corr_point,mse,rmse,mae,smape,r2]\n",
    "\n",
    "def get_confidence_interval_series(sample_array,confidence_level=0.95):\n",
    "    bounds = stats.t.interval(confidence_level,sample_array.shape[0]-1)\n",
    "    samples_mean = np.mean(sample_array,axis=0)\n",
    "    samples_std = np.std(sample_array,axis=0,ddof=1)\n",
    "    lower_bound = samples_mean + bounds[0]*samples_std/np.sqrt(sample_array.shape[0])\n",
    "    upper_bound = samples_mean + bounds[1]*samples_std/np.sqrt(sample_array.shape[0])\n",
    "    return samples_mean, lower_bound, upper_bound\n",
    "\n",
    "def present_mean_metrics(metrics):\n",
    "    print(\"Autocorr\\t\\t MSE\\t\\t RMSE\\t\\t MAE\\t\\t sMAPE\\t\\t R^2\")\n",
    "    print(\"%10.4f\\t %10.4f\\t %10.4f\\t %10.4f\\t %10.4f\\t %10.4f\"% tuple(np.mean(metrics,axis=0)))\n",
    "    print(\"+-\",)\n",
    "    print(\"%10.4f\\t %10.4f\\t %10.4f\\t %10.4f\\t %10.4f\\t %10.4f\"% tuple(np.std(metrics,axis=0,ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'mse'\n",
    "#Dataset parameters\n",
    "window_length = 96\n",
    "forecast_horizon = 24\n",
    "preprocessor = preprocessing.MinMaxScaler()\n",
    "out_preprocessor = preprocessing.MinMaxScaler()\n",
    "# preprocessor = preprocessing.StandardScaler(with_mean=0,with_std=1)\n",
    "# out_preprocessor = preprocessing.StandardScaler(with_mean=0,with_std=1)\n",
    "shuffle_train_set = True\n",
    "scale_output = True\n",
    "training_percentage = 0.75\n",
    "experiment_target = F\"Forecasting,{forecast_horizon} steps ahead\"\n",
    "experiment_complete = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Set up model ##########################\n",
    "class MTCNAModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,tcn_layer_num,tcn_kernel_size,tcn_filter_num,window_size,forecast_horizon, use_bias, kernel_initializer, tcn_dropout_rate,tcn_dropout_format,tcn_activation, tcn_final_activation, tcn_final_stack_activation):\n",
    "        super(MTCNAModel, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.tcn_filter_num = tcn_filter_num\n",
    "        \n",
    "\n",
    "        #Create stack of TCN layers    \n",
    "        self.lower_tcn = TCNStack(tcn_layer_num,tcn_filter_num,tcn_kernel_size,window_size,use_bias,kernel_initializer,tcn_dropout_rate,tcn_dropout_format,tcn_activation,tcn_final_activation,tcn_final_stack_activation)\n",
    "    \n",
    "        #Create stack of dense layers\n",
    "        self.co_seq = tf.keras.models.Sequential()\n",
    "        self.co_seq.add(tf.keras.layers.Flatten())\n",
    "        self.co_seq.add(tf.keras.layers.Dense(forecast_horizon,activation=None))\n",
    "        \n",
    "        #Create stack of dense layers\n",
    "        self.nox_seq = tf.keras.models.Sequential()\n",
    "        self.nox_seq.add(tf.keras.layers.Flatten())\n",
    "        self.nox_seq.add(tf.keras.layers.Dense(forecast_horizon,activation=None))\n",
    "        \n",
    "        #Create stack of dense layers\n",
    "        self.no2_seq = tf.keras.models.Sequential()\n",
    "        self.no2_seq.add(tf.keras.layers.Flatten())\n",
    "        self.no2_seq.add(tf.keras.layers.Dense(forecast_horizon,activation=None))\n",
    "        \n",
    "        #Create stack of dense layers\n",
    "        self.benzene_seq = tf.keras.models.Sequential()\n",
    "        self.benzene_seq.add(tf.keras.layers.Flatten())\n",
    "        self.benzene_seq.add(tf.keras.layers.Dense(forecast_horizon,activation=None))\n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        x = self.lower_tcn(input_tensor)\n",
    "        co = self.co_seq(x)\n",
    "        nox = self.nox_seq(x)\n",
    "        no2 = self.no2_seq(x)\n",
    "        benzene = self.benzene_seq(x)\n",
    "        return co,benzene,nox,no2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Prepare dataset ###########################\n",
    "\n",
    "### Note details for logging purposes\n",
    "dataset_description = \"Italian air quality data\"\n",
    "dataset_preprocessing = \"\"\"Drop time information, Remove NAN rows at end, Replace missing values with 0\"\"\"\n",
    "\n",
    "data = pd.read_csv(\"Datasets/AirQualityUCI.csv\",sep=';',decimal=',')\n",
    "## Remove NaN rows\n",
    "data = data.drop(np.arange(9357,9471,1))\n",
    "# Remove emtpy columns\n",
    "data = data.drop(['Unnamed: 15','Unnamed: 16'],axis=1)\n",
    "\n",
    "\n",
    "#Create date object for easy splitting according to dates\n",
    "dateobj = pd.to_datetime(data[\"Date\"],dayfirst=True) + pd.to_timedelta(data[\"Time\"].str.replace(\".00.00\",\":00:00\"))\n",
    "\n",
    "### For now remove timestamp and output values\n",
    "data = data.drop(columns=[\"Date\",\"Time\"],axis=1)\n",
    "\n",
    "#Drop column due to high number of missing values\n",
    "data = data.drop(['NMHC(GT)'],axis=1)\n",
    "\n",
    "# Replace missing values with 0\n",
    "data = data.replace(-200,0)\n",
    "\n",
    "# Reorganize columns in preparation for second stage (first columns are in order of outputs)\n",
    "columns = ['CO(GT)','C6H6(GT)','NOx(GT)','NO2(GT)','PT08.S1(CO)','PT08.S2(NMHC)','PT08.S3(NOx)','PT08.S4(NO2)','PT08.S5(O3)','T','RH','AH']\n",
    "data = data[columns]\n",
    "\n",
    "\n",
    "## Add date object for splitting\n",
    "data['DateObj'] = dateobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data based on dates\n",
    "training_start_date = pd.Timestamp(year=2004,month=3,day=10)\n",
    "\n",
    "# Preceding values used only for creating final graph and predicting first values of test set\n",
    "holdout_preceding_date = pd.Timestamp(year=2004, month=11, day=11)\n",
    "\n",
    "\n",
    "holdout_set_start_date = pd.Timestamp(year=2004, month=12, day=11)\n",
    "holdout_set_end_date = pd.Timestamp(year=2005, month=4, day=5)\n",
    "\n",
    "training_data = data.loc[(data['DateObj']>=training_start_date) & (data['DateObj'] < holdout_set_start_date)]\n",
    "test_data = data.loc[(data['DateObj'] >= holdout_set_start_date) & (data['DateObj'] < holdout_set_end_date)]\n",
    "\n",
    "pre_evaluation_period = data.loc[(data['DateObj'] >= holdout_preceding_date) & (data['DateObj'] < holdout_set_start_date)]\n",
    "\n",
    "input_variables = list(training_data.columns)\n",
    "\n",
    "training_data = training_data.drop(['DateObj'],axis=1)\n",
    "test_data = test_data.drop(['DateObj'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Select prediction target\n",
    "targets = ['CO(GT)','C6H6(GT)','NOx(GT)','NO2(GT)',]\n",
    "labels = np.array(training_data[targets])\n",
    "\n",
    "\n",
    "if scale_output:\n",
    "    out_preprocessor.fit(labels)\n",
    "    if \"Normalizer\" in str(out_preprocessor.__class__):\n",
    "        ## Save norm so in case of normalizer we can scale the predictions correctly\n",
    "        out_norm = np.linalg.norm(labels)\n",
    "        labels = preprocessing.normalize(labels,axis=0)\n",
    "    else:\n",
    "        labels= out_preprocessor.transform(labels)\n",
    "\n",
    "\n",
    "num_input_time_series = training_data.shape[1]\n",
    "\n",
    "\n",
    "### Make sure data are np arrays in case we skip preprocessing\n",
    "training_data = np.array(training_data)\n",
    "\n",
    "### Fit preprocessor to training data\n",
    "preprocessor.fit(training_data)\n",
    "\n",
    "if \"Normalizer\" in str(preprocessor.__class__):\n",
    "    ## Save norm so in case of normalizer we can scale the test_data correctly\n",
    "    in_norm = np.linalg.norm(training_data,axis=0)\n",
    "    training_data = preprocessing.normalize(training_data,axis=0)\n",
    "else:\n",
    "    training_data = preprocessor.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Create windows for all data\n",
    "data_windows = windowed_dataset(training_data[:-forecast_horizon],num_input_time_series,window_length)\n",
    "label_windows = windowed_forecast(labels[window_length:],forecast_horizon)\n",
    "\n",
    "### Transpose outputs to agree with model output\n",
    "label_windows = np.transpose(label_windows,[0,2,1])\n",
    "\n",
    "\n",
    "samples = data_windows.shape[0]\n",
    "\n",
    "\n",
    "## Shuffle windows\n",
    "if shuffle_train_set:\n",
    "    data_windows, label_windows = shuffle_arrays_together(data_windows,label_windows)\n",
    "\n",
    "### Create train and validation sets\n",
    "train_x = data_windows\n",
    "train_y = [label_windows[:,i,:] for i in range(len(targets))]\n",
    "\n",
    "\n",
    "## In order to use all days of test set for prediction, append training window from preceding period\n",
    "pre_test_train = pre_evaluation_period[test_data.columns][-window_length:]\n",
    "test_data = pd.concat([pre_test_train,test_data])\n",
    "\n",
    "## Create windowed test set with same process\n",
    "test_labels = np.array(test_data[targets])\n",
    "\n",
    "#### Preprocess data\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "if \"Normalizer\" in str(preprocessor.__class__):\n",
    "    test_data = test_data/in_norm\n",
    "else:\n",
    "    test_data = preprocessor.transform(test_data)\n",
    "\n",
    "test_x = windowed_dataset(test_data[:-forecast_horizon],num_input_time_series,window_length)\n",
    "test_y = np.transpose(windowed_forecast(test_labels[window_length:],forecast_horizon),[0,2,1])\n",
    "\n",
    "## Create pre test period for visualization\n",
    "pre_test_target = np.append(np.array(pre_evaluation_period[targets]),test_labels[:window_length])\n",
    "\n",
    "total_samples = train_x.shape[0] + test_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Initialize model parameters ########################\n",
    "## For simplicity all time series TCNs have the same parameters, though it is relatively easy to change this\n",
    "tcn_kernel_size = 3\n",
    "tcn_layer_num = 5\n",
    "tcn_use_bias = True\n",
    "tcn_filter_num = 128\n",
    "tcn_kernel_initializer = 'random_normal'\n",
    "tcn_dropout_rate = 0.3 \n",
    "tcn_dropout_format = \"channel\"\n",
    "tcn_activation = 'relu'\n",
    "tcn_final_activation = 'linear'\n",
    "tcn_final_stack_activation = 'relu'\n",
    "loss = [loss]*len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Check for GPU\n",
    "\n",
    "\n",
    "## Make only given GPU visible   \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "mirrored_strategy = None\n",
    "\n",
    "print(\"GPUs Available: \", gpus)\n",
    "if len(gpus)==0:\n",
    "    device = \"CPU:0\"\n",
    "else:\n",
    "    print(\"Enter number of gpus to use:\")\n",
    "    gpu_num = input()\n",
    "    if len(gpu_num)!=0 and gpu_num.isdigit():\n",
    "        gpu_num = int(gpu_num)\n",
    "    if gpu_num==1:\n",
    "        print(\"Enter index of GPU to use:\")\n",
    "        gpu_idx = input()\n",
    "        if len(gpu_idx)!=0 and gpu_idx.isdigit():\n",
    "            gpu_idx = int(gpu_idx)\n",
    "        tf.config.experimental.set_visible_devices(gpus[gpu_idx], 'GPU')\n",
    "        device = \"GPU:0\"\n",
    "    else:\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy(devices=[F\"GPU:{i}\" for i in range(gpu_num)])\n",
    "        device = \" \".join([F\"GPU:{i}\" for i in range(gpu_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set evaluation seed to affect dropout random execution\n",
    "print(\"Enter a seed for the evaluation:\")\n",
    "seed = input()\n",
    "if len(seed)!=0 and seed.isdigit():\n",
    "    seed = int(seed)\n",
    "else:\n",
    "    seed = 192\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_days.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ## Set up test model\n",
    "## From all the test samples keep individual, non overlapping days\n",
    "test_x_days = test_x[0::forecast_horizon,:]\n",
    "true_y = np.transpose(test_y[0::forecast_horizon,:],(0,2,1)).reshape((-1,len(targets)))\n",
    "\n",
    "test_dropout = 0.3\n",
    "\n",
    "with tf.device(device):\n",
    "    test_model = MTCNAModel(tcn_layer_num,tcn_kernel_size,tcn_filter_num,window_length,forecast_horizon, tcn_use_bias, tcn_kernel_initializer, test_dropout, tcn_dropout_format, tcn_activation, tcn_final_activation, tcn_final_stack_activation)\n",
    "_ = test_model(test_x_days[0:1])\n",
    "\n",
    "\n",
    "    \n",
    "from os import listdir\n",
    "weight_names = listdir(\"BaselineWeights-AirQ/\")\n",
    "print(weight_names)\n",
    "dropout_runs_per_weight = 20\n",
    "\n",
    "\n",
    "metrics_number = 6\n",
    "samples_per_prediction = dropout_runs_per_weight*len(weight_names)\n",
    "\n",
    "## Enable dropout\n",
    "tf.keras.backend.set_learning_phase(1)\n",
    "errors  = np.zeros((samples_per_prediction,test_x_days.shape[0]*forecast_horizon,len(targets)))\n",
    "predictions = np.zeros((samples_per_prediction,test_x_days.shape[0]*forecast_horizon,len(targets)))\n",
    "metrics = np.zeros((samples_per_prediction,metrics_number,len(targets)))\n",
    "\n",
    "for i in tqdm(range(len(weight_names))):\n",
    "    test_model.load_weights(\"BaselineWeights-AirQ/\"+weight_names[i])\n",
    "    for j in range(dropout_runs_per_weight):\n",
    "        cur_pred = np.asarray(test_model(test_x_days)).reshape((len(targets),-1)).T\n",
    "        if scale_output and \"Normalizer\" in str(out_preprocessor.__class__):\n",
    "            cur_pred *= (out_norm)\n",
    "        else:\n",
    "            cur_pred = out_preprocessor.inverse_transform(cur_pred)\n",
    "        predictions[i*dropout_runs_per_weight+j,:] = cur_pred\n",
    "        errors[i*dropout_runs_per_weight+j,:] = cur_pred - true_y\n",
    "        for target in range(len(targets)):\n",
    "            metrics[i*dropout_runs_per_weight+j,:,target] = np.asarray(get_metrics(true_y[:,target],cur_pred[:,target],print_metrics=False))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "for var_idx in range(len(targets)):\n",
    "    print(targets[var_idx])\n",
    "    present_mean_metrics(metrics[...,var_idx])\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    plt.hist((predictions[...,var_idx]-np.median(predictions[...,var_idx],axis=0)).flatten(),bins=20,alpha=0.5,range=(-2,2),label=\"Dropout predictiomn - median\")\n",
    "    plt.hist(errors[...,var_idx].flatten(),alpha=0.5,range=(-5,5),bins=20,label=\"Prediction - Actual\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level = 0.95\n",
    "pred_mean, lower_bound,upper_bound = get_confidence_interval_series(predictions,confidence_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_days.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "preceding_points = 24\n",
    "from_day =50\n",
    "to_day = 51\n",
    "\n",
    "pred_plot_range = range(preceding_points,preceding_points+(to_day-from_day)*forecast_horizon)\n",
    "pred_plot_range1 = range(preceding_points-24,preceding_points+(to_day-from_day)*forecast_horizon-24)\n",
    "pred_sp = from_day*forecast_horizon\n",
    "pred_ep = to_day*forecast_horizon\n",
    "\n",
    "for i in range(len(targets)):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    plt.plot(pred_plot_range,pred_mean[pred_sp:pred_ep,i],marker=\"o\",label=\"Prediction\")\n",
    "    plt.fill_between(pred_plot_range, lower_bound[pred_sp:pred_ep,i], upper_bound[pred_sp:pred_ep,i], alpha=0.3)\n",
    "    \n",
    "    if from_day==0:\n",
    "        plt.plot(pre_test_target[-preceding_points:,i],label=\"Pretest period\", marker=\"o\")\n",
    "    else:\n",
    "        plt.plot(true_y[pred_sp-preceding_points:pred_sp,i],label=\"Pretest period\", marker=\"o\")\n",
    "    plt.plot(pred_plot_range,true_y[from_day*forecast_horizon:to_day*forecast_horizon,i],marker=\"o\",label=\"True data\")\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "#    plt.xticks()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
