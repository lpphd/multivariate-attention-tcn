{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multitcn_components import TCNStack, DownsampleLayerWithAttention, LearningRateLogger\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import tensorflow_addons as tfa\n",
    "import uuid\n",
    "import sys\n",
    "from scipy.signal import correlate\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "from IPython.display import Image\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, time_series_number, window_size):\n",
    "    \"\"\"\n",
    "    Returns a windowed dataset from a Pandas dataframe\n",
    "    \"\"\"\n",
    "    available_examples= series.shape[0]-window_size + 1\n",
    "    time_series_number = series.shape[1]\n",
    "    inputs = np.zeros((available_examples,window_size,time_series_number))\n",
    "    for i in range(available_examples):\n",
    "        inputs[i,:,:] = series[i:i+window_size,:]\n",
    "    return inputs \n",
    "\n",
    "def windowed_forecast(series, forecast_horizon):\n",
    "    available_outputs = series.shape[0]- forecast_horizon + 1\n",
    "    output_series_num = series.shape[1]\n",
    "    output = np.zeros((available_outputs,forecast_horizon, output_series_num))\n",
    "    for i in range(available_outputs):\n",
    "        output[i,:]= series[i:i+forecast_horizon,:]\n",
    "    return output\n",
    "\n",
    "def shuffle_arrays_together(a,b):\n",
    "    p = np.random.permutation(a.shape[0])\n",
    "    return a[p],b[p]\n",
    "\n",
    "def norm_cross_corr(a,b):\n",
    "    nom = correlate(a,b)\n",
    "    den = np.sqrt(np.sum(np.power(a,2))*np.sum(np.power(b,2)))\n",
    "    return nom/den\n",
    "\n",
    "def symm_mape(true,prediction):\n",
    "    return 100*np.sum(2*np.abs(prediction-true)/(np.abs(true)+np.abs(prediction)))/true.size\n",
    "\n",
    "def get_metrics(true,prediction,print_metrics=False):\n",
    "        c = norm_cross_corr(true,prediction)\n",
    "        extent = int((c.shape[0]-1)/2)\n",
    "        max_corr_point = np.argmax(c)-extent\n",
    "        max_corr = np.max(c)\n",
    "        max_v = np.max(prediction)\n",
    "        mse = mean_squared_error(true,prediction,squared=True)\n",
    "        rmse = mean_squared_error(true,prediction,squared=False)\n",
    "        mae = mean_absolute_error(true,prediction)\n",
    "        r2 = r2_score(true,prediction)\n",
    "        smape = symm_mape(true,prediction)\n",
    "        if print_metrics:\n",
    "            print(\"Max %f - Autocorr %d - MSE %f - RMSE %f - MAE %f - sMAPE %f%% - R^2 %f\"%(max_v,max_corr_point,mse,rmse,mae,smape,r2))\n",
    "        return [max_corr_point,mse,rmse,mae,smape,r2]\n",
    "\n",
    "def get_confidence_interval_series(sample_array,confidence_level=0.95):\n",
    "    bounds = stats.t.interval(confidence_level,sample_array.shape[0]-1)\n",
    "    samples_mean = np.mean(sample_array,axis=0)\n",
    "    samples_std = np.std(sample_array,axis=0,ddof=1)\n",
    "    lower_bound = samples_mean + bounds[0]*samples_std/np.sqrt(sample_array.shape[0])\n",
    "    upper_bound = samples_mean + bounds[1]*samples_std/np.sqrt(sample_array.shape[0])\n",
    "    return samples_mean, lower_bound, upper_bound\n",
    "\n",
    "def present_mean_metrics(metrics):\n",
    "    print(\"Autocorr\\t\\t MSE\\t\\t RMSE\\t\\t MAE\\t\\t sMAPE\\t\\t R^2\")\n",
    "    print(\"%10.4f\\t %10.4f\\t %10.4f\\t %10.4f\\t %10.4f\\t %10.4f\"% tuple(np.mean(metrics,axis=0)))\n",
    "    print(\"+-\",)\n",
    "    print(\"%10.4f\\t %10.4f\\t %10.4f\\t %10.4f\\t %10.4f\\t %10.4f\"% tuple(np.std(metrics,axis=0,ddof=1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'mse'\n",
    "#Dataset parameters\n",
    "window_length = 96\n",
    "forecast_horizon = 24\n",
    "preprocessor = preprocessing.MinMaxScaler()\n",
    "out_preprocessor = preprocessing.MinMaxScaler()\n",
    "# preprocessor = preprocessing.StandardScaler(with_mean=0,with_std=1)\n",
    "# out_preprocessor = preprocessing.StandardScaler(with_mean=0,with_std=1)\n",
    "shuffle_train_set = True\n",
    "scale_output = True\n",
    "training_percentage = 0.75\n",
    "experiment_target = F\"Forecasting,{forecast_horizon} steps ahead\"\n",
    "experiment_complete = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Set up model ##########################\n",
    "class MTCNAModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, tcn_layer_num,tcn_kernel_size,tcn_filter_num,window_size,forecast_horizon,num_output_time_series, use_bias, kernel_initializer, tcn_dropout_rate,tcn_dropout_format,tcn_activation, tcn_final_activation, tcn_final_stack_activation):\n",
    "        super(MTCNAModel, self).__init__()\n",
    "\n",
    "\n",
    "        self.num_output_time_series = num_output_time_series\n",
    "        \n",
    "\n",
    "        #Create stack of TCN layers    \n",
    "        self.lower_tcn = TCNStack(tcn_layer_num,tcn_filter_num,tcn_kernel_size,window_size,use_bias,kernel_initializer,tcn_dropout_rate,tcn_dropout_format,tcn_activation,tcn_final_activation, tcn_final_stack_activation)\n",
    "        \n",
    "        self.downsample_att = DownsampleLayerWithAttention(num_output_time_series,window_size, tcn_kernel_size, forecast_horizon, kernel_initializer, None)\n",
    "    \n",
    "        \n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        x = self.lower_tcn(input_tensor)\n",
    "        x, distribution = self.downsample_att([x,input_tensor[:,:,:self.num_output_time_series]])\n",
    "        return [x[:,i,:] for i in range(self.num_output_time_series)], distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Prepare dataset ###########################\n",
    "\n",
    "### Note details for logging purposes\n",
    "dataset_description = \"Italian air quality data\"\n",
    "dataset_preprocessing = \"\"\"Drop time information, Remove NAN rows at end, Replace missing values with 0\"\"\"\n",
    "\n",
    "data = pd.read_csv(\"Datasets/AirQualityUCI.csv\",sep=';',decimal=',')\n",
    "## Remove NaN rows\n",
    "data = data.drop(np.arange(9357,9471,1))\n",
    "# Remove emtpy columns\n",
    "data = data.drop(['Unnamed: 15','Unnamed: 16'],axis=1)\n",
    "\n",
    "\n",
    "#Create date object for easy splitting according to dates\n",
    "dateobj = pd.to_datetime(data[\"Date\"],dayfirst=True) + pd.to_timedelta(data[\"Time\"].str.replace(\".00.00\",\":00:00\"))\n",
    "\n",
    "### For now remove timestamp and output values\n",
    "data = data.drop(columns=[\"Date\",\"Time\"],axis=1)\n",
    "\n",
    "#Drop column due to high number of missing values\n",
    "data = data.drop(['NMHC(GT)'],axis=1)\n",
    "\n",
    "# Replace missing values with 0\n",
    "data = data.replace(-200,0)\n",
    "\n",
    "# Reorganize columns in preparation for second stage (first columns are in order of outputs)\n",
    "columns = ['CO(GT)','C6H6(GT)','NOx(GT)','NO2(GT)','PT08.S1(CO)','PT08.S2(NMHC)','PT08.S3(NOx)','PT08.S4(NO2)','PT08.S5(O3)','T','RH','AH']\n",
    "data = data[columns]\n",
    "\n",
    "\n",
    "## Add date object for splitting\n",
    "data['DateObj'] = dateobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data based on dates\n",
    "training_start_date = pd.Timestamp(year=2004,month=3,day=10)\n",
    "\n",
    "# Preceding values used only for creating final graph and predicting first values of test set\n",
    "holdout_preceding_date = pd.Timestamp(year=2004, month=11, day=11)\n",
    "\n",
    "\n",
    "holdout_set_start_date = pd.Timestamp(year=2004, month=12, day=11)\n",
    "holdout_set_end_date = pd.Timestamp(year=2005, month=4, day=5)\n",
    "\n",
    "training_data = data.loc[(data['DateObj']>=training_start_date) & (data['DateObj'] < holdout_set_start_date)]\n",
    "test_data = data.loc[(data['DateObj'] >= holdout_set_start_date) & (data['DateObj'] < holdout_set_end_date)]\n",
    "\n",
    "pre_evaluation_period = data.loc[(data['DateObj'] >= holdout_preceding_date) & (data['DateObj'] < holdout_set_start_date)]\n",
    "\n",
    "input_variables = list(training_data.columns)\n",
    "\n",
    "training_data = training_data.drop(['DateObj'],axis=1)\n",
    "test_data = test_data.drop(['DateObj'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Select prediction target\n",
    "targets = ['CO(GT)','C6H6(GT)','NOx(GT)','NO2(GT)']\n",
    "labels = np.array(training_data[targets])\n",
    "\n",
    "\n",
    "if scale_output:\n",
    "    out_preprocessor.fit(labels)\n",
    "    if \"Normalizer\" in str(out_preprocessor.__class__):\n",
    "        ## Save norm so in case of normalizer we can scale the predictions correctly\n",
    "        out_norm = np.linalg.norm(labels)\n",
    "        labels = preprocessing.normalize(labels,axis=0)\n",
    "    else:\n",
    "        labels= out_preprocessor.transform(labels)\n",
    "\n",
    "\n",
    "num_input_time_series = training_data.shape[1]\n",
    "\n",
    "\n",
    "### Make sure data are np arrays in case we skip preprocessing\n",
    "training_data = np.array(training_data)\n",
    "\n",
    "### Fit preprocessor to training data\n",
    "preprocessor.fit(training_data)\n",
    "\n",
    "if \"Normalizer\" in str(preprocessor.__class__):\n",
    "    ## Save norm so in case of normalizer we can scale the test_data correctly\n",
    "    in_norm = np.linalg.norm(training_data,axis=0)\n",
    "    training_data = preprocessing.normalize(training_data,axis=0)\n",
    "else:\n",
    "    training_data = preprocessor.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Create windows for all data\n",
    "data_windows = windowed_dataset(training_data[:-forecast_horizon],num_input_time_series,window_length)\n",
    "label_windows = windowed_forecast(labels[window_length:],forecast_horizon)\n",
    "\n",
    "### Transpose outputs to agree with model output\n",
    "label_windows = np.transpose(label_windows,[0,2,1])\n",
    "\n",
    "\n",
    "samples = data_windows.shape[0]\n",
    "\n",
    "\n",
    "## Shuffle windows\n",
    "if shuffle_train_set:\n",
    "    data_windows, label_windows = shuffle_arrays_together(data_windows,label_windows)\n",
    "\n",
    "### Create train and validation sets\n",
    "train_x = data_windows\n",
    "train_y = [label_windows[:,i,:] for i in range(len(targets))]\n",
    "\n",
    "\n",
    "## In order to use all days of test set for prediction, append training window from preceding period\n",
    "pre_test_train = pre_evaluation_period[test_data.columns][-window_length:]\n",
    "test_data = pd.concat([pre_test_train,test_data])\n",
    "\n",
    "## Create windowed test set with same process\n",
    "test_labels = np.array(test_data[targets])\n",
    "\n",
    "#### Preprocess data\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "if \"Normalizer\" in str(preprocessor.__class__):\n",
    "    test_data = test_data/in_norm\n",
    "else:\n",
    "    test_data = preprocessor.transform(test_data)\n",
    "\n",
    "test_x = windowed_dataset(test_data[:-forecast_horizon],num_input_time_series,window_length)\n",
    "test_y = np.transpose(windowed_forecast(test_labels[window_length:],forecast_horizon),[0,2,1])\n",
    "\n",
    "## Create pre test period for visualization\n",
    "pre_test_target = np.append(np.array(pre_evaluation_period[targets]),test_labels[:window_length])\n",
    "\n",
    "total_samples = train_x.shape[0] + test_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Initialize model parameters ########################\n",
    "## For simplicity all time series TCNs have the same parameters, though it is relatively easy to change this\n",
    "tcn_kernel_size = 3\n",
    "tcn_layer_num = 5\n",
    "tcn_use_bias = True\n",
    "tcn_filter_num = 128\n",
    "tcn_kernel_initializer = 'random_normal'\n",
    "tcn_dropout_rate = 0.3 \n",
    "tcn_dropout_format = \"channel\"\n",
    "tcn_activation = 'relu'\n",
    "tcn_final_activation = 'linear'\n",
    "tcn_final_stack_activation = 'relu'\n",
    "loss = [loss]*len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Check for GPU\n",
    "\n",
    "## Make only given GPU visible   \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "mirrored_strategy = None\n",
    "\n",
    "print(\"GPUs Available: \", gpus)\n",
    "if len(gpus)==0:\n",
    "    device = \"CPU:0\"\n",
    "else:\n",
    "    print(\"Enter number of gpus to use:\")\n",
    "    gpu_num = input()\n",
    "    if len(gpu_num)!=0 and gpu_num.isdigit():\n",
    "        gpu_num = int(gpu_num)\n",
    "    if gpu_num==1:\n",
    "        print(\"Enter index of GPU to use:\")\n",
    "        gpu_idx = input()\n",
    "        if len(gpu_idx)!=0 and gpu_idx.isdigit():\n",
    "            gpu_idx = int(gpu_idx)\n",
    "        tf.config.experimental.set_visible_devices(gpus[gpu_idx], 'GPU')\n",
    "        device = \"GPU:0\"\n",
    "    else:\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy(devices=[F\"GPU:{i}\" for i in range(gpu_num)])\n",
    "        device = \" \".join([F\"GPU:{i}\" for i in range(gpu_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set evaluation seed to affect dropout random execution\n",
    "print(\"Enter a seed for the evaluation:\")\n",
    "seed = input()\n",
    "if len(seed)!=0 and seed.isdigit():\n",
    "    seed = int(seed)\n",
    "else:\n",
    "    seed = 192\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up test model\n",
    "## From all the test samples keep individual, non overlapping days\n",
    "test_x_days = test_x[0::forecast_horizon,:]\n",
    "true_y = np.transpose(test_y[0::forecast_horizon,:],(0,2,1)).reshape((-1,len(targets)))\n",
    "\n",
    "test_dropout = 0.3\n",
    "\n",
    "with tf.device(device):\n",
    "    test_model = MTCNAModel(tcn_layer_num,tcn_kernel_size,tcn_filter_num,window_length,forecast_horizon,len(targets), tcn_use_bias, tcn_kernel_initializer, test_dropout, tcn_dropout_format, tcn_activation, tcn_final_activation, tcn_final_stack_activation)\n",
    "_ = test_model(train_x[0:1])\n",
    "\n",
    "\n",
    "best_weight_name = \"510e465d-c041-4fb3-b76c-f514fde218ae-weights.112-0.0282.h5\"\n",
    "\n",
    "## Generate predictions for test set using best weight (first in list)\n",
    "## Reset training fase to disable dropout \n",
    "tf.keras.backend.set_learning_phase(0)\n",
    "test_model.load_weights(\"SecondStageWeights-AirQ/\"+best_weight_name)\n",
    "\n",
    "best_pred = np.asarray(test_model(test_x_days)[0]).reshape((len(targets),-1)).T\n",
    "if scale_output and \"Normalizer\" in str(out_preprocessor.__class__):\n",
    "    best_pred *= (out_norm)\n",
    "else:\n",
    "    best_pred = out_preprocessor.inverse_transform(best_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from os import listdir\n",
    "weight_names = listdir(\"SecondStageWeights-AirQ/\")\n",
    "dropout_runs_per_weight = 20\n",
    "\n",
    "metrics_number = 6\n",
    "samples_per_prediction = dropout_runs_per_weight*len(weight_names)\n",
    "\n",
    "## Enable dropout\n",
    "tf.keras.backend.set_learning_phase(1)\n",
    "\n",
    "dl_errors  = np.zeros((samples_per_prediction,test_x_days.shape[0]*forecast_horizon,len(targets)))\n",
    "dl_predictions = np.zeros((samples_per_prediction,test_x_days.shape[0]*forecast_horizon,len(targets)))\n",
    "dl_metrics = np.zeros((samples_per_prediction,metrics_number,len(targets)))\n",
    "\n",
    "for i in tqdm(range(len(weight_names))):\n",
    "    test_model.load_weights(\"SecondStageWeights-AirQ/\"+weight_names[i])\n",
    "    for j in range(dropout_runs_per_weight):\n",
    "        ## Get DL test set predictions and metrics\n",
    "        cur_pred = np.asarray(test_model(test_x_days)[0]).reshape((len(targets),-1)).T\n",
    "        if scale_output and \"Normalizer\" in str(out_preprocessor.__class__):\n",
    "            cur_pred *= (out_norm)\n",
    "        else:\n",
    "            cur_pred = out_preprocessor.inverse_transform(cur_pred)\n",
    "        dl_predictions[i*dropout_runs_per_weight+j,:] = cur_pred\n",
    "        dl_errors[i*dropout_runs_per_weight+j,:] = cur_pred - true_y\n",
    "        for t in range(len(targets)):\n",
    "            dl_metrics[i*dropout_runs_per_weight+j,:,t] = np.asarray(get_metrics(true_y[:,t],cur_pred[:,t],print_metrics=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=100)\n",
    "sns.set()\n",
    "for var_idx in range(len(targets)):\n",
    "    print(targets[var_idx])\n",
    "    present_mean_metrics(dl_metrics[...,var_idx])\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    plt.hist(dl_errors[...,var_idx].flatten(),alpha=0.5)\n",
    "    plt.hist((dl_predictions[...,var_idx]-np.median(dl_predictions[...,var_idx],axis=0)).flatten(),alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean, dl_lower_bound, dl_upper_bound = get_confidence_interval_series(dl_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preceding_points = 24\n",
    "from_day = 10\n",
    "to_day = 20\n",
    "\n",
    "pred_plot_range = range(preceding_points,preceding_points+(to_day-from_day)*forecast_horizon)\n",
    "pred_sp = from_day*forecast_horizon\n",
    "pred_ep = to_day*forecast_horizon\n",
    "\n",
    "for i in range(len(targets)):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    plt.plot(pred_plot_range,pred_mean[pred_sp:pred_ep,i],marker=\"o\",label=\"Prediction\")\n",
    "    plt.fill_between(pred_plot_range, dl_lower_bound[pred_sp:pred_ep,i], dl_upper_bound[pred_sp:pred_ep,i], alpha=0.3)\n",
    "    \n",
    "    if from_day==0:\n",
    "        plt.plot(pre_test_target[-preceding_points:,i],label=\"Pretest period\", marker=\"o\")\n",
    "    else:\n",
    "        plt.plot(true_y[pred_sp-preceding_points:pred_sp,i],label=\"Pretest period\", marker=\"o\")\n",
    "    plt.plot(pred_plot_range,true_y[from_day*forecast_horizon:to_day*forecast_horizon,i],marker=\"o\",label=\"True data\")\n",
    "\n",
    "    plt.grid(axis='x')\n",
    "    plt.legend()\n",
    "    plt.title(targets[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Present attention graphs for specific prediction output\n",
    "\n",
    "input_variables = ['CO(GT)','C6H6(GT)','NOx(GT)','NO2(GT)']\n",
    "\n",
    "var_of_interest = 'C6H6(GT)'\n",
    "\n",
    "var_idx = input_variables.index(var_of_interest)\n",
    "\n",
    "test_idx = 45\n",
    "\n",
    "## Reset training fase to disable dropout \n",
    "tf.keras.backend.set_learning_phase(0)\n",
    "test_model.load_weights(\"SecondStageWeights-AirQ/\"+best_weight_name)\n",
    "\n",
    "\n",
    "o, dist = test_model(test_x_days[test_idx:test_idx+1])\n",
    "\n",
    "o = np.asarray(o).reshape((len(targets),-1)).T\n",
    "if scale_output:\n",
    "    if \"Normalizer\" in str(out_preprocessor.__class__):\n",
    "        o *= (out_norm)\n",
    "    else:\n",
    "        o = out_preprocessor.inverse_transform(o)\n",
    "        \n",
    "inp = preprocessor.inverse_transform(test_x_days[test_idx])[:,var_idx]\n",
    "\n",
    "prediction= o[:,var_idx]\n",
    "true_out = true_y[test_idx*forecast_horizon:(test_idx+1)*(forecast_horizon),var_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(figsize=(20,10))\n",
    "plt.plot(inp)\n",
    "plt.plot(np.arange(window_length,window_length+forecast_horizon),prediction,marker=\"o\",label=\"Prediction\")\n",
    "plt.plot(np.arange(window_length,window_length+forecast_horizon),true_out,marker=\"o\",label=\"Ground truth\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get value dense layer\n",
    "for w in test_model.weights:\n",
    "    if w.name.endswith(\"sep_dense_value_weights:0\"):\n",
    "        weights = np.abs(w.numpy())[var_idx]\n",
    "        #weights = w.numpy()[var_idx]\n",
    "        break\n",
    "\n",
    "dist_var = dist.numpy()[0,var_idx,...]\n",
    "full_dist = np.matmul(dist_var,weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "def infl_to_out_elem(out_elem):\n",
    "    elem_dist = full_dist[out_elem:out_elem+1,:]\n",
    "    prep = preprocessing.MinMaxScaler()\n",
    "    prep.fit(elem_dist.T)\n",
    "    elem_dist = prep.transform(elem_dist.T)\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    sns.heatmap(elem_dist.T, cmap=\"Blues\", cbar=True, yticklabels=False, xticklabels=10)\n",
    "    ax2 = plt.twinx()\n",
    "    ax2.plot(range(window_length,window_length+forecast_horizon),true_out,label=\"Ground truth\",marker=\"o\")\n",
    "    ax2.plot(range(window_length,window_length+forecast_horizon),prediction,label=\"Prediction\",marker=\"o\")\n",
    "    plt.plot([window_length+out_elem], [prediction[out_elem]], marker='o', label= \"Step \"+str(out_elem+1), markersize=8, color=\"black\")\n",
    "    sns.lineplot(x=np.arange(0,window_length),y=inp, ax=ax2)\n",
    "    ax.axis('tight')\n",
    "    ax2.legend(fontsize=20)\n",
    "    plt.show()\n",
    "#     plt.savefig(\"dist_images/%s-%02d.png\"%(var_of_interest,out_elem))\n",
    "#     plt.close(fig)\n",
    "\n",
    "interact(infl_to_out_elem, out_elem=(0,forecast_horizon-1,1))\n",
    "#infl_to_out_elem(12)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
