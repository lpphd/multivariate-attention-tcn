{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multitcn_components import TCNStack, DownsampleLayerWithAttention, LearningRateLogger\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import tensorflow_addons as tfa\n",
    "import uuid\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set experiment seed\n",
    "print(\"Enter a seed for the experiment:\")\n",
    "seed = input()\n",
    "if len(seed)!=0 and seed.isdigit():\n",
    "    seed = int(seed)\n",
    "else:\n",
    "    seed = 192\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_experiment_details(just_print=False, complete=False):\n",
    "    \"\"\" Log experiment details \"\"\"\n",
    "    if just_print:\n",
    "        o = sys.stdout\n",
    "    else:\n",
    "        o = open(date_time_string+\"--\"+experiment_id+\".txt\",\"w\")\n",
    "    o.write(\"Date and time: %s \\n\" % date_time_string)\n",
    "    o.write(\"Experiment complete : %s\\n\" % str(experiment_complete))\n",
    "    if complete:\n",
    "        now = datetime.now()\n",
    "        completion_time = now.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "        o.write(\"Experiment complete at: %s\\n\" % completion_time)\n",
    "        o.write(\"Duration in seconds: %f \\n\"% duration)\n",
    "    o.write(\"Filename: %s\\n\" % str(sys.argv[0]))\n",
    "    o.write(\"\\n\\n\")\n",
    "    o.write(\" Training parameters \".center(100,\"=\"))\n",
    "    o.write(\"\\n\\n\")\n",
    "    o.write(\"Batch size: %d\\n\" % batch_size)\n",
    "    o.write(\"Epochs : %d\\n\" % epochs)\n",
    "    o.write(\"Device used: %s\\n\" % device)\n",
    "    o.write(\"Random seed used: %s\\n\" % seed)\n",
    "    o.write(\"Loss : %s\\n\" % loss)\n",
    "    o.write(\"Optimizer config: %s\" % str(optimizer.get_config()))\n",
    "    o.write(\"\\n\\n\")\n",
    "    o.write(\" Dataset parameters \".center(100,\"=\"))\n",
    "    o.write(\"\\n\\n\")\n",
    "    o.write(\"Dataset description: %s\\n\" % dataset_description)\n",
    "    o.write(\"Number of input time series: %d\\n\" % num_input_time_series)\n",
    "    o.write(\"Window length: %d\\n\" % window_length)\n",
    "    o.write(\"Total sample size: %d \\n\"% total_samples)\n",
    "    o.write(\"Training samples: %d\\n\" % int(train_x.shape[0]*training_percentage))\n",
    "    o.write(\"Training start date: %s\\n\" % training_start_date)\n",
    "    o.write(\"Validation samples: %d\\n\" % int(train_x.shape[0]*(1-training_percentage)))\n",
    "    o.write(\"Test samples: %d\\n\" % test_x.shape[0])\n",
    "    o.write(\"Test start date: %s\\n\" % holdout_set_start_date)\n",
    "    o.write(\"Test end date: %s\\n\" % holdout_set_end_date)\n",
    "    o.write(\"Experiment target: %s\\n\" % experiment_target)\n",
    "    o.write(\"Dataset preprocessing: %s\\n\" % dataset_preprocessing)\n",
    "    o.write(\"Shuffled training and val set: %s\\n\" % shuffle_train_set)\n",
    "    o.write(\"Scaled output: %s\\n\" % scale_output)\n",
    "    o.write(\"Input preprocessor details: %s, %s\\n\" %(preprocessor.__class__, preprocessor.get_params()))\n",
    "    o.write(\"Output scaler details: %s, %s\\n\" %(out_preprocessor.__class__, out_preprocessor.get_params()))\n",
    "    o.write(\"\\n\\n\")\n",
    "    o.write(\" Specific model parameters \".center(100,\"=\"))\n",
    "    o.write(\"\\n\\n\")\n",
    "    o.write(\"tcn_kernel_size : %d\\n\" % tcn_kernel_size)\n",
    "    o.write(\"tcn_filter_num : %d\\n\" % tcn_filter_num)\n",
    "    o.write(\"tcn_layer_num : %d\\n\" % tcn_layer_num)\n",
    "    o.write(\"tcn_use_bias : %s\\n\" % str(tcn_use_bias))\n",
    "    o.write(\"tcn_kernel_initializer : %s\\n\" % tcn_kernel_initializer)\n",
    "    o.write(\"tcn_dropout_rate : %0.2f\\n\" % tcn_dropout_rate)\n",
    "    o.write(\"tcn_dropout_format : %s\\n\" % tcn_dropout_format)\n",
    "    o.write(\"tcn_activation : %s\\n\" % tcn_activation)\n",
    "    o.write(\"tcn_final_activation : %s\\n\" % tcn_final_activation)\n",
    "    o.write(\"tcn_final_stack_activation : %s\\n\" % tcn_final_stack_activation)\n",
    "    o.write(\"\\n\\n\")\n",
    "    o.write(\" Additional useful notes \".center(100,\"=\"))\n",
    "    o.write(\"\\n\\n\")\n",
    "    o.write(additional_experiment_notes)\n",
    "    o.write(\"\\n\\n\")\n",
    "    \n",
    "    if not just_print:\n",
    "        o.close()\n",
    "    else:\n",
    "        o.flush()\n",
    "\n",
    "def windowed_dataset(series, time_series_number, window_size):\n",
    "    \"\"\"\n",
    "    Returns a windowed dataset from a Pandas dataframe\n",
    "    \"\"\"\n",
    "    available_examples= series.shape[0]-window_size + 1\n",
    "    time_series_number = series.shape[1]\n",
    "    inputs = np.zeros((available_examples,window_size,time_series_number))\n",
    "    for i in range(available_examples):\n",
    "        inputs[i,:,:] = series[i:i+window_size,:]\n",
    "    return inputs \n",
    "\n",
    "def windowed_forecast(series, forecast_horizon):\n",
    "    available_outputs = series.shape[0]- forecast_horizon + 1\n",
    "    output_series_num = series.shape[1]\n",
    "    output = np.zeros((available_outputs,forecast_horizon, output_series_num))\n",
    "    for i in range(available_outputs):\n",
    "        output[i,:]= series[i:i+forecast_horizon,:]\n",
    "    return output\n",
    "\n",
    "def shuffle_arrays_together(a,b):\n",
    "    p = np.random.permutation(a.shape[0])\n",
    "    return a[p],b[p]\n",
    "\n",
    "def remove_outliers_and_interpolate(dataframe, std_times = 3):\n",
    "    \"\"\"\n",
    "    Removes outliers further than std_times standard deviations from the mean of each column of a df and replaces them with simple interpolated values\n",
    "    \"\"\"\n",
    "    for c in ['Temp_degC']:\n",
    "        mask = (dataframe>40)\n",
    "        dataframe.loc[mask[c],c] = np.nan\n",
    "\n",
    "    for c in ['Turbidity_NTU','Chloraphylla_ugL']:\n",
    "        mask = (dataframe<0)\n",
    "        dataframe.loc[mask[c],c] = np.nan\n",
    "\n",
    "    for c in list(dataframe.columns):\n",
    "        mean = np.mean(np.array(dataframe[c]))\n",
    "        std = np.std(np.array(dataframe[c]))\n",
    "        mask =((dataframe < (mean - std_times*std)) | (dataframe > (mean+std_times*std)))\n",
    "        dataframe.loc[mask[c],c] = np.nan\n",
    "    \n",
    "    dataframe = dataframe.interpolate()\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Set up experiment parameters ###############\n",
    "\n",
    "#Logging parameters\n",
    "experiment_id = str(uuid.uuid4())\n",
    "now = datetime.now()\n",
    "date_time_string = now.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "# For potential tensorboard use\n",
    "log_dir=\"logs/profile/\" + date_time_string\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch = 3)\n",
    "\n",
    "#Training parameters\n",
    "epochs = 120\n",
    "batch_size = 64\n",
    "starting_lr = 1e-3\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=starting_lr, weight_decay=1e-4)\n",
    "min_lr = 2e-6\n",
    "loss ='mse'\n",
    "    \n",
    "    \n",
    "log_name=\"logs/\"+ F\"{experiment_id}-{date_time_string}_train_history\"\n",
    "log_history_callback = CSVLogger(log_name)\n",
    "filepath= experiment_id+\"-weights.{epoch:02d}-{val_loss:.4f}.h5\"\n",
    "save_model_callback = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "# Unused callbacks after all\n",
    "#lr_log_name = \"logs/\"+ F\"{experiment_id}-{date_time_string}_learning_rate_history\"\n",
    "#lr_log_callback = LearningRateLogger(lr_log_name)\n",
    "#lr_reducer_callback = ReduceLROnPlateau(factor=0.5,cooldown=0,patience=20,min_delta=0.001,min_lr=min_lr, verbose=1)\n",
    "#lr_schedule = LearningRateScheduler(lambda epoch, lr: 1e-8 * 10**(epoch / 20))\n",
    "#early_stopping_callback = EarlyStopping(monitor='val_loss', patience=40, min_delta=0.001)\n",
    "\n",
    "callbacks_list = [log_history_callback, save_model_callback]#,lr_log_callback, lr_reducer_callback, early_stopping_callback]\n",
    "\n",
    "#Dataset parameters\n",
    "window_length = 192\n",
    "forecast_horizon = 48\n",
    "preprocessor = preprocessing.MinMaxScaler()\n",
    "out_preprocessor = preprocessing.MinMaxScaler()\n",
    "shuffle_train_set = True\n",
    "scale_output = True\n",
    "training_percentage = 0.9\n",
    "experiment_target = F\"Forecasting,{forecast_horizon} steps ahead\"\n",
    "experiment_complete = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Set up model ##########################\n",
    "class MTCNAModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, tcn_layer_num,tcn_kernel_size,tcn_filter_num,window_size,forecast_horizon,num_output_time_series, use_bias, kernel_initializer, tcn_dropout_rate,tcn_dropout_format,tcn_activation, tcn_final_activation, tcn_final_stack_activation):\n",
    "        super(MTCNAModel, self).__init__()\n",
    "\n",
    "\n",
    "        self.num_output_time_series = num_output_time_series\n",
    "        \n",
    "\n",
    "        #Create stack of TCN layers    \n",
    "        self.lower_tcn = TCNStack(tcn_layer_num,tcn_filter_num,tcn_kernel_size,window_size,use_bias,kernel_initializer,tcn_dropout_rate,tcn_dropout_format,tcn_activation,tcn_final_activation, tcn_final_stack_activation)\n",
    "        \n",
    "        self.downsample_att = DownsampleLayerWithAttention(num_output_time_series,window_size, tcn_kernel_size, forecast_horizon, kernel_initializer, None)\n",
    "    \n",
    "        \n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        x = self.lower_tcn(input_tensor)\n",
    "        x, distribution = self.downsample_att([x,input_tensor])\n",
    "        return [x[:,i,:] for i in range(self.num_output_time_series)]#, distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Prepare dataset ###########################\n",
    "\n",
    "### Note details for logging purposes\n",
    "dataset_description = \"Burnett river sensor data\"\n",
    "dataset_preprocessing = \"\"\"Drop TIMESTAMP, Replace outliers more than 3*std on input data with Nan,\n",
    "pd.interpolate() for NaN values\"\"\"\n",
    "\n",
    "# Read csv in pandas\n",
    "data_files = []\n",
    "\n",
    "for year in range(2014,2019):\n",
    "    data_file = pd.read_csv(F\"Datasets/burnett-river-trailer-quality-{year}.csv\")\n",
    "    data_files.append(data_file)\n",
    "data = pd.concat(data_files,axis=0)\n",
    "\n",
    "# Change type of temp to avoid errors\n",
    "data = data.astype({'Temp_degC':'float64'})\n",
    "\n",
    "#Create date object for easy splitting according to dates\n",
    "dateobj = pd.to_datetime(data['TIMESTAMP'])\n",
    "\n",
    "### For now remove timestamp and output valuesdata = remove_outliers_and_interpolate(data, std_times=3)\n",
    "data = data.drop(columns=[\"TIMESTAMP\",\"RECORD\"],axis=1)\n",
    " \n",
    "data = remove_outliers_and_interpolate(data, std_times=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add date object for splitting\n",
    "data['DateObj'] = dateobj\n",
    "\n",
    "#Split data based on dates\n",
    "training_start_date = pd.Timestamp(year=2014,month=3,day=1)\n",
    "\n",
    "# Preceding values used only for creating final graph and predicting first values of test set\n",
    "holdout_preceding_date = pd.Timestamp(year=2017, month=3, day=1)\n",
    "holdout_set_start_date = pd.Timestamp(year=2017, month=4, day=1)\n",
    "holdout_set_end_date = pd.Timestamp(year=2018, month=4, day=1)\n",
    "\n",
    "training_data = data.loc[(data['DateObj']>=training_start_date) & (data['DateObj'] < holdout_set_start_date)]\n",
    "test_data = data.loc[(data['DateObj'] >= holdout_set_start_date) & (data['DateObj'] < holdout_set_end_date)]\n",
    "pre_evaluation_period = data.loc[(data['DateObj'] >= holdout_preceding_date) & (data['DateObj'] < holdout_set_start_date)]\n",
    "\n",
    "## Keep iput variables\n",
    "input_variables = ['Temp_degC', 'EC_uScm', 'pH', 'Turbidity_NTU', 'Chloraphylla_ugL']\n",
    "\n",
    "training_data = training_data[input_variables]\n",
    "test_data = test_data[input_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Select prediction target\n",
    "targets = ['Temp_degC', 'EC_uScm', 'pH', 'Turbidity_NTU','Chloraphylla_ugL']\n",
    "labels = np.array(training_data[targets])\n",
    "\n",
    "if scale_output:\n",
    "    out_preprocessor.fit(labels)\n",
    "    if \"Normalizer\" in str(out_preprocessor.__class__):\n",
    "        ## Save norm so in case of normalizer we can scale the predictions correctly\n",
    "        out_norm = np.linalg.norm(labels)\n",
    "        labels = preprocessing.normalize(labels,axis=0)\n",
    "    else:\n",
    "        labels= out_preprocessor.transform(labels)\n",
    "\n",
    "\n",
    "num_input_time_series = training_data.shape[1]\n",
    "\n",
    "\n",
    "### Make sure data are np arrays in case we skip preprocessing\n",
    "training_data = np.array(training_data)\n",
    "\n",
    "# #### Fit preprocessor to training data\n",
    "preprocessor.fit(training_data)\n",
    "\n",
    "if \"Normalizer\" in str(preprocessor.__class__):\n",
    "    ## Save norm so in case of normalizer we can scale the test_data correctly\n",
    "    in_norm = np.linalg.norm(training_data,axis=0)\n",
    "    training_data = preprocessing.normalize(training_data,axis=0)\n",
    "else:\n",
    "    training_data = preprocessor.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Create windows for all data\n",
    "data_windows = windowed_dataset(training_data[:-forecast_horizon],num_input_time_series,window_length)\n",
    "label_windows = windowed_forecast(labels[window_length:],forecast_horizon)\n",
    "\n",
    "### Transpose outputs to agree with model output\n",
    "label_windows = np.transpose(label_windows,[0,2,1])\n",
    "\n",
    "\n",
    "samples = data_windows.shape[0]\n",
    "\n",
    "\n",
    "## Shuffle windows\n",
    "if shuffle_train_set:\n",
    "    data_windows, label_windows = shuffle_arrays_together(data_windows,label_windows)\n",
    "\n",
    "### Create train and validation sets\n",
    "train_x = data_windows\n",
    "train_y = [label_windows[:,i,:] for i in range(len(targets))]\n",
    "\n",
    "\n",
    "## In order to use all days of test set for prediction, append training window from preceding period\n",
    "pre_test_train = pre_evaluation_period[test_data.columns][-window_length:]\n",
    "test_data = pd.concat([pre_test_train,test_data])\n",
    "\n",
    "## Create windowed test set with same process\n",
    "test_labels = np.array(test_data[targets])\n",
    "\n",
    "#### Preprocess data\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "if \"Normalizer\" in str(preprocessor.__class__):\n",
    "    test_data = test_data/in_norm\n",
    "else:\n",
    "    test_data = preprocessor.transform(test_data)\n",
    "\n",
    "test_x = windowed_dataset(test_data[:-forecast_horizon],num_input_time_series,window_length)\n",
    "test_y = np.transpose(windowed_forecast(test_labels[window_length:],forecast_horizon),[0,2,1])\n",
    "\n",
    "## Create pre test period for visualization\n",
    "pre_test_target = np.vstack((np.array(pre_evaluation_period[targets]),test_labels[:window_length]))\n",
    "\n",
    "total_samples = train_x.shape[0] + test_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Initialize model parameters ########################\n",
    "## For simplicity all time series TCNs have the same parameters, though it is relatively easy to change this\n",
    "tcn_kernel_size = 3\n",
    "tcn_layer_num = 7\n",
    "tcn_use_bias = True\n",
    "tcn_filter_num = 64\n",
    "tcn_kernel_initializer = 'random_normal'\n",
    "tcn_dropout_rate = 0.5\n",
    "tcn_dropout_format = \"channel\"\n",
    "tcn_activation = 'relu'\n",
    "tcn_final_activation = 'linear'\n",
    "tcn_final_stack_activation = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Check for GPU\n",
    "\n",
    "\n",
    "## Make only given GPU visible   \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "mirrored_strategy = None\n",
    "\n",
    "print(\"GPUs Available: \", gpus)\n",
    "if len(gpus)==0:\n",
    "    device = \"CPU:0\"\n",
    "else:\n",
    "    print(\"Enter number of gpus to use:\")\n",
    "    gpu_num = input()\n",
    "    if len(gpu_num)!=0 and gpu_num.isdigit():\n",
    "        gpu_num = int(gpu_num)\n",
    "    if gpu_num==1:\n",
    "        print(\"Enter index of GPU to use:\")\n",
    "        gpu_idx = input()\n",
    "        if len(gpu_idx)!=0 and gpu_idx.isdigit():\n",
    "            gpu_idx = int(gpu_idx)\n",
    "        tf.config.experimental.set_visible_devices(gpus[gpu_idx], 'GPU')\n",
    "        device = \"GPU:0\"\n",
    "    else:\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy(devices=[F\"GPU:{i}\" for i in range(gpu_num)])\n",
    "        device = \" \".join([F\"GPU:{i}\" for i in range(gpu_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Initialize model\n",
    "loss = [loss]*len(targets)\n",
    "if mirrored_strategy:\n",
    "    with mirrored_strategy.scope():\n",
    "        model = MTCNAModel(tcn_layer_num,tcn_kernel_size,tcn_filter_num,window_length,forecast_horizon,len(targets), tcn_use_bias, tcn_kernel_initializer, tcn_dropout_rate, tcn_dropout_format, tcn_activation, tcn_final_activation, tcn_final_stack_activation)\n",
    "        model.compile(optimizer,loss,metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "else:\n",
    "    with tf.device(device):\n",
    "        model = MTCNAModel(tcn_layer_num,tcn_kernel_size,tcn_filter_num,window_length,forecast_horizon,len(targets), tcn_use_bias, tcn_kernel_initializer, tcn_dropout_rate, tcn_dropout_format, tcn_activation, tcn_final_activation, tcn_final_stack_activation)\n",
    "        model.compile(optimizer,loss,metrics=[tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_experiment_notes = \"\"\n",
    "log_experiment_details(just_print=True)\n",
    "############## Note here any special notes about this experiment\n",
    "print(\"Do you have any special notes for this experiment ? If yes, please write them here (or press Enter):\")\n",
    "additional_experiment_notes = input()\n",
    "if len(additional_experiment_notes)==0:\n",
    "    additional_experiment_notes = \"\"\"No special notes\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create log and start training\n",
    "\n",
    "log_experiment_details()\n",
    "\n",
    "start_time = tf.timestamp()\n",
    "\n",
    "history = model.fit(x=train_x, y= train_y,validation_split=(1-training_percentage), batch_size=batch_size, epochs=epochs, callbacks=callbacks_list)\n",
    "\n",
    "end_time = tf.timestamp()\n",
    "\n",
    "duration = end_time - start_time\n",
    "\n",
    "experiment_complete = True\n",
    "log_experiment_details(complete=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
