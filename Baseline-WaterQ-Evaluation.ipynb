{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multitcn_components import TCNStack, DownsampleLayerWithAttention, LearningRateLogger\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import tensorflow_addons as tfa\n",
    "import uuid\n",
    "import sys\n",
    "from scipy.signal import correlate\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, time_series_number, window_size):\n",
    "    \"\"\"\n",
    "    Returns a windowed dataset from a Pandas dataframe\n",
    "    \"\"\"\n",
    "    available_examples= series.shape[0]-window_size + 1\n",
    "    time_series_number = series.shape[1]\n",
    "    inputs = np.zeros((available_examples,window_size,time_series_number))\n",
    "    for i in range(available_examples):\n",
    "        inputs[i,:,:] = series[i:i+window_size,:]\n",
    "    return inputs \n",
    "\n",
    "def windowed_forecast(series, forecast_horizon):\n",
    "    available_outputs = series.shape[0]- forecast_horizon + 1\n",
    "    output_series_num = series.shape[1]\n",
    "    output = np.zeros((available_outputs,forecast_horizon, output_series_num))\n",
    "    for i in range(available_outputs):\n",
    "        output[i,:]= series[i:i+forecast_horizon,:]\n",
    "    return output\n",
    "\n",
    "def shuffle_arrays_together(a,b):\n",
    "    p = np.random.permutation(a.shape[0])\n",
    "    return a[p],b[p]\n",
    "\n",
    "def remove_outliers_and_interpolate(dataframe, std_times = 3):\n",
    "    \"\"\"\n",
    "    Removes outliers further than std_times standard deviations from the mean of each column of a df and replaces them with simple interpolated values\n",
    "    \"\"\"\n",
    "    for c in ['Temp_degC']:\n",
    "        mask = (dataframe>40)\n",
    "        dataframe.loc[mask[c],c] = np.nan\n",
    "\n",
    "    for c in ['Turbidity_NTU','Chloraphylla_ugL']:\n",
    "        mask = (dataframe<0)\n",
    "        dataframe.loc[mask[c],c] = np.nan\n",
    "\n",
    "    for c in list(dataframe.columns):\n",
    "        mean = np.mean(np.array(dataframe[c]))\n",
    "        std = np.std(np.array(dataframe[c]))\n",
    "        mask =((dataframe < (mean - std_times*std)) | (dataframe > (mean+std_times*std)))\n",
    "        dataframe.loc[mask[c],c] = np.nan\n",
    "    \n",
    "    dataframe = dataframe.interpolate()\n",
    "    return dataframe\n",
    "\n",
    "def norm_cross_corr(a,b):\n",
    "    nom = correlate(a,b)\n",
    "    den = np.sqrt(np.sum(np.power(a,2))*np.sum(np.power(b,2)))\n",
    "    return nom/den\n",
    "\n",
    "def symm_mape(true,prediction):\n",
    "    return 100*np.sum(2*np.abs(prediction-true)/(np.abs(true)+np.abs(prediction)))/true.size\n",
    "\n",
    "def get_metrics(true,prediction,print_metrics=False):\n",
    "        c = norm_cross_corr(true,prediction)\n",
    "        extent = int((c.shape[0]-1)/2)\n",
    "        max_corr_point = np.argmax(c)-extent\n",
    "        max_corr = np.max(c)\n",
    "        max_v = np.max(prediction)\n",
    "        mse = mean_squared_error(true,prediction,squared=True)\n",
    "        rmse = mean_squared_error(true,prediction,squared=False)\n",
    "        mae = mean_absolute_error(true,prediction)\n",
    "        r2 = r2_score(true,prediction)\n",
    "        smape = symm_mape(true,prediction)\n",
    "        if print_metrics:\n",
    "            print(\"Max %f - Autocorr %d - MSE %f - RMSE %f - MAE %f - sMAPE %f%% - R^2 %f\"%(max_v,max_corr_point,mse,rmse,mae,smape,r2))\n",
    "        return [max_corr_point,mse,rmse,mae,smape,r2]\n",
    "\n",
    "def get_confidence_interval_series(sample_array,confidence_level=0.95):\n",
    "    bounds = stats.t.interval(confidence_level,sample_array.shape[0]-1)\n",
    "    samples_mean = np.mean(sample_array,axis=0)\n",
    "    samples_std = np.std(sample_array,axis=0,ddof=1)\n",
    "    lower_bound = samples_mean + bounds[0]*samples_std/np.sqrt(sample_array.shape[0])\n",
    "    upper_bound = samples_mean + bounds[1]*samples_std/np.sqrt(sample_array.shape[0])\n",
    "    return samples_mean, lower_bound, upper_bound\n",
    "\n",
    "def present_mean_metrics(metrics):\n",
    "    print(\"Autocorr\\t\\t MSE\\t\\t RMSE\\t\\t MAE\\t\\t sMAPE\\t\\t R^2\")\n",
    "    print(\"%10.4f\\t %10.4f\\t %10.4f\\t %10.4f\\t %10.4f\\t %10.4f\"% tuple(np.mean(metrics,axis=0)))\n",
    "    print(\"+-\",)\n",
    "    print(\"%10.4f\\t %10.4f\\t %10.4f\\t %10.4f\\t %10.4f\\t %10.4f\"% tuple(np.std(metrics,axis=0,ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ['mse','mse']\n",
    "\n",
    "#Dataset parameters\n",
    "window_length = 192\n",
    "forecast_horizon = 48\n",
    "preprocessor = preprocessing.MinMaxScaler()\n",
    "out_preprocessor = preprocessing.MinMaxScaler()\n",
    "shuffle_train_set = True\n",
    "scale_output = True\n",
    "training_percentage = 0.9\n",
    "experiment_target = F\"Forecasting,{forecast_horizon} steps ahead\"\n",
    "experiment_complete = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Set up model ##########################\n",
    "class MTCNAModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,tcn_layer_num,tcn_kernel_size,tcn_filter_num,window_size,forecast_horizon, use_bias, kernel_initializer, tcn_dropout_rate,tcn_dropout_format,tcn_activation, tcn_final_activation, tcn_final_stack_activation):\n",
    "        super(MTCNAModel, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.tcn_filter_num = tcn_filter_num\n",
    "        \n",
    "\n",
    "        #Create stack of TCN layers    \n",
    "        self.lower_tcn = TCNStack(tcn_layer_num,tcn_filter_num,tcn_kernel_size,window_size,use_bias,kernel_initializer,tcn_dropout_rate,tcn_dropout_format,tcn_activation,tcn_final_activation,tcn_final_stack_activation)\n",
    "    \n",
    "        #Create stack of dense layers\n",
    "        self.oxy_seq = tf.keras.models.Sequential()\n",
    "        self.oxy_seq.add(tf.keras.layers.Dense(64,activation='relu'))\n",
    "        self.oxy_seq.add(tf.keras.layers.Flatten())\n",
    "        self.oxy_seq.add(tf.keras.layers.Dense(forecast_horizon,activation=None))\n",
    "\n",
    "        #Create stack of dense layers\n",
    "        self.temp_seq = tf.keras.models.Sequential()\n",
    "        self.temp_seq.add(tf.keras.layers.Dense(64,activation='relu'))\n",
    "        self.temp_seq.add(tf.keras.layers.Flatten())\n",
    "        self.temp_seq.add(tf.keras.layers.Dense(forecast_horizon,activation=None))\n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        x = self.lower_tcn(input_tensor)\n",
    "        oxy = self.oxy_seq(x)\n",
    "        temp = self.temp_seq(x)\n",
    "        return oxy,temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Prepare dataset ###########################\n",
    "\n",
    "# Read csv in pandas\n",
    "data_files = []\n",
    "\n",
    "for year in range(2014,2019):\n",
    "    data_file = pd.read_csv(F\"Datasets/burnett-river-trailer-quality-{year}.csv\")\n",
    "    data_files.append(data_file)\n",
    "data = pd.concat(data_files,axis=0)\n",
    "\n",
    "# Change type of temp to avoid errors\n",
    "data = data.astype({'Temp_degC':'float64'})\n",
    "\n",
    "#Create date object for easy splitting according to dates\n",
    "dateobj = pd.to_datetime(data['TIMESTAMP'])\n",
    "\n",
    "### For now remove timestamp and output valuesdata = remove_outliers_and_interpolate(data, std_times=3)\n",
    "data = data.drop(columns=[\"TIMESTAMP\",\"RECORD\"],axis=1)\n",
    "         \n",
    "\n",
    "data = remove_outliers_and_interpolate(data, std_times=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add date object for splitting\n",
    "data['DateObj'] = dateobj\n",
    "\n",
    "#Split data based on dates\n",
    "training_start_date = pd.Timestamp(year=2014,month=3,day=1)\n",
    "\n",
    "# Preceding values used only for creating final graph and predicting first values of test set\n",
    "holdout_preceding_date = pd.Timestamp(year=2017, month=3, day=1)\n",
    "holdout_set_start_date = pd.Timestamp(year=2017, month=4, day=1)\n",
    "holdout_set_end_date = pd.Timestamp(year=2018, month=4, day=1)\n",
    "\n",
    "training_data = data.loc[(data['DateObj']>=training_start_date) & (data['DateObj'] < holdout_set_start_date)]\n",
    "test_data = data.loc[(data['DateObj'] >= holdout_set_start_date) & (data['DateObj'] < holdout_set_end_date)]\n",
    "pre_evaluation_period = data.loc[(data['DateObj'] >= holdout_preceding_date) & (data['DateObj'] < holdout_set_start_date)]\n",
    "\n",
    "## Remove no longer needed columns\n",
    "training_data = training_data.drop(['DO_Sat','DateObj'],axis=1)\n",
    "\n",
    "input_variables = list(training_data.columns)\n",
    "\n",
    "test_data = test_data.drop(['DO_Sat','DateObj'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Select prediction target\n",
    "targets = ['DO_mg','Temp_degC']\n",
    "labels = np.array(training_data[targets])\n",
    "if scale_output:\n",
    "    out_preprocessor.fit(labels)\n",
    "    if \"Normalizer\" in str(out_preprocessor.__class__):\n",
    "        ## Save norm so in case of normalizer we can scale the predictions correctly\n",
    "        out_norm = np.linalg.norm(labels,axis=0)\n",
    "        labels = preprocessing.normalize(labels,axis=0)\n",
    "    else:\n",
    "        labels= out_preprocessor.transform(labels)\n",
    "\n",
    "\n",
    "num_input_time_series = training_data.shape[1]\n",
    "\n",
    "\n",
    "### Make sure data are np arrays in case we skip preprocessing\n",
    "training_data = np.array(training_data)\n",
    "\n",
    "# #### Fit preprocessor to training data\n",
    "preprocessor.fit(training_data)\n",
    "\n",
    "if \"Normalizer\" in str(preprocessor.__class__):\n",
    "    ## Save norm so in case of normalizer we can scale the test_data correctly\n",
    "    in_norm = np.linalg.norm(training_data,axis=0)\n",
    "    training_data = preprocessing.normalize(training_data,axis=0)\n",
    "else:\n",
    "    training_data = preprocessor.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Create windows for all data\n",
    "data_windows = windowed_dataset(training_data[:-forecast_horizon],num_input_time_series,window_length)\n",
    "label_windows = windowed_forecast(labels[window_length:],forecast_horizon)\n",
    "\n",
    "### Transpose outputs to agree with model output\n",
    "label_windows = np.transpose(label_windows,[0,2,1])\n",
    "\n",
    "\n",
    "samples = data_windows.shape[0]\n",
    "\n",
    "\n",
    "## Shuffle windows\n",
    "if shuffle_train_set:\n",
    "    data_windows, label_windows = shuffle_arrays_together(data_windows,label_windows)\n",
    "\n",
    "### Create train and validation sets\n",
    "train_x = data_windows\n",
    "train_y = [label_windows[:,i,:] for i in range(len(targets))]\n",
    "\n",
    "\n",
    "## In order to use all days of test set for prediction, append training window from preceding period\n",
    "pre_test_train = pre_evaluation_period[test_data.columns][-window_length:]\n",
    "test_data = pd.concat([pre_test_train,test_data])\n",
    "\n",
    "## Create windowed test set with same process\n",
    "test_labels = np.array(test_data[targets])\n",
    "\n",
    "#### Preprocess data\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "if \"Normalizer\" in str(preprocessor.__class__):\n",
    "    test_data = test_data/in_norm\n",
    "else:\n",
    "    test_data = preprocessor.transform(test_data)\n",
    "\n",
    "test_x = windowed_dataset(test_data[:-forecast_horizon],num_input_time_series,window_length)\n",
    "test_y = np.transpose(windowed_forecast(test_labels[window_length:],forecast_horizon),[0,2,1])\n",
    "\n",
    "## Create pre test period for visualization\n",
    "pre_test_target = np.vstack((np.array(pre_evaluation_period[targets]),test_labels[:window_length]))\n",
    "\n",
    "total_samples = train_x.shape[0] + test_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Initialize model parameters ########################\n",
    "## For simplicity all time series TCNs have the same parameters, though it is relatively easy to change this\n",
    "tcn_kernel_size = 3\n",
    "tcn_layer_num = 7\n",
    "tcn_use_bias = True\n",
    "tcn_filter_num = 64\n",
    "tcn_kernel_initializer = 'random_normal'\n",
    "tcn_dropout_rate = 0.5\n",
    "tcn_dropout_format = \"channel\"\n",
    "tcn_activation = 'relu'\n",
    "tcn_final_activation = 'linear'\n",
    "tcn_final_stack_activation = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check for GPU\n",
    "gpus = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", gpus)\n",
    "if gpus==0:\n",
    "    device = \"CPU:0\"\n",
    "else:\n",
    "    device = \"GPU:0\"\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set evaluation seed to affect dropout random execution\n",
    "print(\"Enter a seed for the evaluation:\")\n",
    "seed = input()\n",
    "if len(seed)!=0 and seed.isdigit():\n",
    "    seed = int(seed)\n",
    "else:\n",
    "    seed = 192\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ## Set up test model\n",
    "## From all the test samples keep individual, non overlapping days\n",
    "test_x_days = test_x[0::forecast_horizon,:]\n",
    "true_y = np.transpose(test_y[0::forecast_horizon,:],(0,2,1)).reshape((-1,len(targets)))\n",
    "\n",
    "test_dropout = 0.85\n",
    "\n",
    "with tf.device(device):\n",
    "    test_model = MTCNAModel(tcn_layer_num,tcn_kernel_size,tcn_filter_num,window_length,forecast_horizon, tcn_use_bias, tcn_kernel_initializer, test_dropout, tcn_dropout_format, tcn_activation, tcn_final_activation, tcn_final_stack_activation)\n",
    "_ = test_model(test_x_days[0:1])\n",
    "\n",
    "\n",
    "    \n",
    "from os import listdir\n",
    "weight_names = listdir(\"BaselineWeights-WaterQ/\")\n",
    "print(weight_names)\n",
    "dropout_runs_per_weight = 20\n",
    "\n",
    "\n",
    "metrics_number = 6\n",
    "samples_per_prediction = dropout_runs_per_weight*len(weight_names)\n",
    "\n",
    "## Enable dropout\n",
    "tf.keras.backend.set_learning_phase(1)\n",
    "errors  = np.zeros((samples_per_prediction,test_x_days.shape[0]*forecast_horizon,len(targets)))\n",
    "predictions = np.zeros((samples_per_prediction,test_x_days.shape[0]*forecast_horizon,len(targets)))\n",
    "metrics = np.zeros((samples_per_prediction,metrics_number,len(targets)))\n",
    "\n",
    "for i in tqdm(range(len(weight_names))):\n",
    "    test_model.load_weights(\"BaselineWeights-WaterQ/\"+weight_names[i])\n",
    "    for j in range(dropout_runs_per_weight):\n",
    "        cur_pred = np.asarray(test_model(test_x_days)).reshape((len(targets),-1)).T\n",
    "        if scale_output and \"Normalizer\" in str(out_preprocessor.__class__):\n",
    "            cur_pred *= (out_norm)\n",
    "        else:\n",
    "            cur_pred = out_preprocessor.inverse_transform(cur_pred)\n",
    "        predictions[i*dropout_runs_per_weight+j,:] = cur_pred\n",
    "        errors[i*dropout_runs_per_weight+j,:] = cur_pred - true_y\n",
    "        for target in range(len(targets)):\n",
    "            metrics[i*dropout_runs_per_weight+j,:,target] = np.asarray(get_metrics(true_y[:,target],cur_pred[:,target],print_metrics=False))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "for var_idx in range(len(targets)):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    print(targets[var_idx])\n",
    "    present_mean_metrics(metrics[...,var_idx])\n",
    "    plt.hist((predictions[...,var_idx]-np.median(predictions[...,var_idx],axis=0)).flatten(),bins=20,alpha=0.5,range=(-2,2),label=\"Dropout predictiomn - median\")\n",
    "    plt.hist(errors[...,var_idx].flatten(),alpha=0.5,range=(-2,2),bins=20,label=\"Prediction - Actual\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level = 0.95\n",
    "pred_mean, lower_bound,upper_bound = get_confidence_interval_series(predictions,confidence_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = holdout_set_start_date.to_pydatetime()\n",
    "\n",
    "preceding_points = 24\n",
    "from_day = 19\n",
    "to_day = 20\n",
    "d1 = d0 + timedelta(days=from_day)\n",
    "\n",
    "pred_plot_range = range(preceding_points,preceding_points+(to_day-from_day)*forecast_horizon)\n",
    "pred_sp = from_day*forecast_horizon\n",
    "pred_ep = to_day*forecast_horizon\n",
    "\n",
    "for i in range(len(targets)):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    plt.plot(pred_plot_range,pred_mean[pred_sp:pred_ep,i],marker=\"o\",label=\"Prediction\")\n",
    "    plt.fill_between(pred_plot_range, lower_bound[pred_sp:pred_ep,i], upper_bound[pred_sp:pred_ep,i], alpha=0.3)\n",
    "    \n",
    "    if from_day==0:\n",
    "        plt.plot(pre_test_target[-preceding_points:,i],label=\"Pretest period\", marker=\"o\")\n",
    "    else:\n",
    "        plt.plot(true_y[pred_sp-preceding_points:pred_sp,i],label=\"Pretest period\", marker=\"o\")\n",
    "    plt.plot(pred_plot_range,true_y[from_day*forecast_horizon:to_day*forecast_horizon,i],marker=\"o\",label=\"True data\")\n",
    "\n",
    "    plt.grid(axis='x')\n",
    "    #plt.ylim(top=7.6)\n",
    "    plt.legend()\n",
    "    plt.xlabel(d1.strftime(\"%d/%m/%Y\"))\n",
    "    plt.ylabel([\"mg/L\",\"Â°C\"][i])\n",
    "    plt.xticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
