{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multitcn_components import TCNStack, DownsampleLayerWithAttention, LearningRateLogger\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime,date,timedelta\n",
    "import tensorflow_addons as tfa\n",
    "import uuid\n",
    "import sys\n",
    "from scipy.signal import correlate\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "import eli5\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pygam import LinearGAM, s, f, te, GAM\n",
    "from pygam import terms\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree, export_graphviz\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, time_series_number, window_size):\n",
    "    \"\"\"\n",
    "    Returns a windowed dataset from a Pandas dataframe\n",
    "    \"\"\"\n",
    "    available_examples= series.shape[0]-window_size + 1\n",
    "    time_series_number = series.shape[1]\n",
    "    inputs = np.zeros((available_examples,window_size,time_series_number))\n",
    "    for i in range(available_examples):\n",
    "        inputs[i,:,:] = series[i:i+window_size,:]\n",
    "    return inputs \n",
    "\n",
    "def windowed_forecast(series, forecast_horizon):\n",
    "    available_outputs = series.shape[0]- forecast_horizon + 1\n",
    "    output_series_num = series.shape[1]\n",
    "    output = np.zeros((available_outputs,forecast_horizon, output_series_num))\n",
    "    for i in range(available_outputs):\n",
    "        output[i,:]= series[i:i+forecast_horizon,:]\n",
    "    return output\n",
    "\n",
    "def shuffle_arrays_together(a,b):\n",
    "    p = np.random.permutation(a.shape[0])\n",
    "    return a[p],b[p]\n",
    "\n",
    "def remove_outliers_and_interpolate(dataframe, std_times = 3):\n",
    "    \"\"\"\n",
    "    Removes outliers further than std_times standard deviations from the mean of each column of a df and replaces them with simple interpolated values\n",
    "    \"\"\"\n",
    "    for c in ['Temp_degC']:\n",
    "        mask = (dataframe>40)\n",
    "        dataframe.loc[mask[c],c] = np.nan\n",
    "\n",
    "    for c in ['Turbidity_NTU','Chloraphylla_ugL']:\n",
    "        mask = (dataframe<0)\n",
    "        dataframe.loc[mask[c],c] = np.nan\n",
    "\n",
    "    for c in list(dataframe.columns):\n",
    "        mean = np.mean(np.array(dataframe[c]))\n",
    "        std = np.std(np.array(dataframe[c]))\n",
    "        mask =((dataframe < (mean - std_times*std)) | (dataframe > (mean+std_times*std)))\n",
    "        dataframe.loc[mask[c],c] = np.nan\n",
    "    \n",
    "    dataframe = dataframe.interpolate()\n",
    "    return dataframe\n",
    "\n",
    "def norm_cross_corr(a,b):\n",
    "    nom = correlate(a,b)\n",
    "    den = np.sqrt(np.sum(np.power(a,2))*np.sum(np.power(b,2)))\n",
    "    return nom/den\n",
    "\n",
    "def symm_mape(true,prediction):\n",
    "    return 100*np.sum(2*np.abs(prediction-true)/(np.abs(true)+np.abs(prediction)))/true.size\n",
    "\n",
    "def get_metrics(true,prediction,print_metrics=False):\n",
    "        c = norm_cross_corr(true,prediction)\n",
    "        extent = int((c.shape[0]-1)/2)\n",
    "        max_corr_point = np.argmax(c)-extent\n",
    "        max_corr = np.max(c)\n",
    "        max_v = np.max(prediction)\n",
    "        mse = mean_squared_error(true,prediction,squared=True)\n",
    "        rmse = mean_squared_error(true,prediction,squared=False)\n",
    "        mae = mean_absolute_error(true,prediction)\n",
    "        r2 = r2_score(true,prediction)\n",
    "        smape = symm_mape(true,prediction)\n",
    "        if print_metrics:\n",
    "            print(\"Max %f - Autocorr %d - MSE %f - RMSE %f - MAE %f - sMAPE %f%% - R^2 %f\"%(max_v,max_corr_point,mse,rmse,mae,smape,r2))\n",
    "        return [max_corr_point,mse,rmse,mae,smape,r2]\n",
    "\n",
    "def get_confidence_interval_series(sample_array,confidence_level=0.95):\n",
    "    bounds = stats.t.interval(confidence_level,sample_array.shape[0]-1)\n",
    "    samples_mean = np.mean(sample_array,axis=0)\n",
    "    samples_std = np.std(sample_array,axis=0,ddof=1)\n",
    "    lower_bound = samples_mean + bounds[0]*samples_std/np.sqrt(sample_array.shape[0])\n",
    "    upper_bound = samples_mean + bounds[1]*samples_std/np.sqrt(sample_array.shape[0])\n",
    "    return samples_mean, lower_bound, upper_bound\n",
    "\n",
    "def present_mean_metrics(metrics):\n",
    "    print(\"Autocorr\\t\\t MSE\\t\\t RMSE\\t\\t MAE\\t\\t sMAPE\\t\\t R^2\")\n",
    "    print(\"%10.4f\\t %10.4f\\t %10.4f\\t %10.4f\\t %10.4f\\t %10.4f\"% tuple(np.mean(metrics,axis=0)))\n",
    "    print(\"+-\",)\n",
    "    print(\"%10.4f\\t %10.4f\\t %10.4f\\t %10.4f\\t %10.4f\\t %10.4f\"% tuple(np.std(metrics,axis=0,ddof=1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Set up experiment parameters ###############\n",
    "#Training parameters\n",
    "loss ='mse'\n",
    "\n",
    "#Dataset parameters\n",
    "window_length = 192\n",
    "forecast_horizon = 48\n",
    "preprocessor = preprocessing.MinMaxScaler()\n",
    "out_preprocessor = preprocessing.MinMaxScaler()\n",
    "shuffle_train_set = True\n",
    "scale_output = True\n",
    "training_percentage = 0.9\n",
    "experiment_target = F\"Forecasting,{forecast_horizon} steps ahead\"\n",
    "experiment_complete = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Set up model ##########################\n",
    "class MTCNAModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, tcn_layer_num,tcn_kernel_size,tcn_filter_num,window_size,forecast_horizon,num_output_time_series, use_bias, kernel_initializer, tcn_dropout_rate,tcn_dropout_format,tcn_activation, tcn_final_activation, tcn_final_stack_activation):\n",
    "        super(MTCNAModel, self).__init__()\n",
    "\n",
    "\n",
    "        self.num_output_time_series = num_output_time_series\n",
    "        \n",
    "\n",
    "        #Create stack of TCN layers    \n",
    "        self.lower_tcn = TCNStack(tcn_layer_num,tcn_filter_num,tcn_kernel_size,window_size,use_bias,kernel_initializer,tcn_dropout_rate,tcn_dropout_format,tcn_activation,tcn_final_activation, tcn_final_stack_activation)\n",
    "        \n",
    "        self.downsample_att = DownsampleLayerWithAttention(num_output_time_series,window_size, tcn_kernel_size, forecast_horizon, kernel_initializer, None)\n",
    "        \n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        x = self.lower_tcn(input_tensor)\n",
    "        x, distribution = self.downsample_att([x,input_tensor])\n",
    "        return [x[:,i,:] for i in range(self.num_output_time_series)], distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Prepare dataset ###########################\n",
    "\n",
    "### Note details for logging purposes\n",
    "dataset_description = \"Burnett river sensor data\"\n",
    "dataset_preprocessing = \"\"\"Drop TIMESTAMP, Replace outliers more than 3*std on input data with Nan,\n",
    "pd.interpolate() for NaN values\"\"\"\n",
    "\n",
    "# Read csv in pandas\n",
    "data_files = []\n",
    "\n",
    "for year in range(2014,2019):\n",
    "    data_file = pd.read_csv(F\"Datasets/burnett-river-trailer-quality-{year}.csv\")\n",
    "    data_files.append(data_file)\n",
    "data = pd.concat(data_files,axis=0)\n",
    "\n",
    "# Change type of temp to avoid errors\n",
    "data = data.astype({'Temp_degC':'float64'})\n",
    "\n",
    "#Create date object for easy splitting according to dates\n",
    "dateobj = pd.to_datetime(data['TIMESTAMP'])\n",
    "\n",
    "### For now remove timestamp and output outliers\n",
    "data = data.drop(columns=[\"TIMESTAMP\",\"RECORD\"],axis=1)\n",
    "\n",
    "data = remove_outliers_and_interpolate(data, std_times=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add date object for splitting\n",
    "data['DateObj'] = dateobj\n",
    "\n",
    "#Split data based on dates\n",
    "training_start_date = pd.Timestamp(year=2014,month=3,day=1)\n",
    "\n",
    "# Preceding values used only for creating final graph and predicting first values of test set\n",
    "holdout_preceding_date = pd.Timestamp(year=2017, month=3, day=1)\n",
    "holdout_set_start_date = pd.Timestamp(year=2017, month=4, day=1)\n",
    "holdout_set_end_date = pd.Timestamp(year=2018, month=4, day=1)\n",
    "\n",
    "training_data = data.loc[(data['DateObj']>=training_start_date) & (data['DateObj'] < holdout_set_start_date)]\n",
    "test_data = data.loc[(data['DateObj'] >= holdout_set_start_date) & (data['DateObj'] < holdout_set_end_date)]\n",
    "pre_evaluation_period = data.loc[(data['DateObj'] >= holdout_preceding_date) & (data['DateObj'] < holdout_set_start_date)]\n",
    "\n",
    "\n",
    "## Keep iput variables\n",
    "input_variables = ['Temp_degC', 'EC_uScm', 'pH', 'Turbidity_NTU', 'Chloraphylla_ugL'] # 'DO_mg', 'DO_Sat'\n",
    "#input_variables = ['Temp_degC', 'EC_uScm', 'pH', 'DO_mg', 'Turbidity_NTU', 'Chloraphylla_ugL']\n",
    "\n",
    "## Save for explainable models training\n",
    "do_and_date = training_data[[\"DO_mg\",\"DateObj\"]].copy()\n",
    "do_and_date_test = test_data[[\"DO_mg\",\"DateObj\"]].copy()\n",
    "\n",
    "training_data = training_data[input_variables]\n",
    "test_data = test_data[input_variables+[\"DO_mg\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Select prediction target\n",
    "targets = ['Temp_degC', 'EC_uScm', 'pH', 'Turbidity_NTU','Chloraphylla_ugL']\n",
    "labels = np.array(training_data[targets])\n",
    "\n",
    "if scale_output:\n",
    "    out_preprocessor.fit(labels)\n",
    "    if \"Normalizer\" in str(out_preprocessor.__class__):\n",
    "        ## Save norm so in case of normalizer we can scale the predictions correctly\n",
    "        out_norm = np.linalg.norm(labels)\n",
    "        labels = preprocessing.normalize(labels,axis=0)\n",
    "    else:\n",
    "        labels= out_preprocessor.transform(labels)\n",
    "\n",
    "\n",
    "num_input_time_series = training_data.shape[1]\n",
    "\n",
    "\n",
    "### Make sure data are np arrays in case we skip preprocessing\n",
    "training_data = np.array(training_data)\n",
    "\n",
    "\n",
    "#### Fit preprocessor to training data\n",
    "preprocessor.fit(training_data)\n",
    "\n",
    "if \"Normalizer\" in str(preprocessor.__class__):\n",
    "    ## Save norm so in case of normalizer we can scale the test_data correctly\n",
    "    in_norm = np.linalg.norm(training_data,axis=0)\n",
    "    training_data = preprocessing.normalize(training_data,axis=0)\n",
    "else:\n",
    "    training_data = preprocessor.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Create windows for all data\n",
    "data_windows = windowed_dataset(training_data[:-forecast_horizon],num_input_time_series,window_length)\n",
    "label_windows = windowed_forecast(labels[window_length:],forecast_horizon)\n",
    "\n",
    "### Transpose outputs to agree with model output\n",
    "label_windows = np.transpose(label_windows,[0,2,1])\n",
    "\n",
    "\n",
    "samples = data_windows.shape[0]\n",
    "\n",
    "unshuffled_data_windows = data_windows.copy()\n",
    "\n",
    "## Shuffle windows\n",
    "if shuffle_train_set:\n",
    "    data_windows, label_windows = shuffle_arrays_together(data_windows,label_windows)\n",
    "\n",
    "### Create train and validation sets\n",
    "train_x = data_windows\n",
    "train_y = [label_windows[:,i,:] for i in range(len(targets))]\n",
    "\n",
    "\n",
    "## In order to use all days of test set for prediction, append training window from preceding period\n",
    "pre_test_train = pre_evaluation_period[test_data.columns][-window_length:]\n",
    "test_data = pd.concat([pre_test_train,test_data])\n",
    "\n",
    "test_data = test_data[input_variables]\n",
    "## Create windowed test set with same process\n",
    "test_labels = np.array(test_data[targets])\n",
    "\n",
    "#### Preprocess data\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "if \"Normalizer\" in str(preprocessor.__class__):\n",
    "    test_data = test_data/in_norm\n",
    "else:\n",
    "    test_data = preprocessor.transform(test_data)\n",
    "\n",
    "test_x = windowed_dataset(test_data[:-forecast_horizon],num_input_time_series,window_length)\n",
    "test_y = np.transpose(windowed_forecast(test_labels[window_length:],forecast_horizon),[0,2,1])\n",
    "\n",
    "\n",
    "\n",
    "## Create pre test period for visualization\n",
    "pre_test_target = np.vstack((np.array(pre_evaluation_period[targets]),test_labels[:window_length]))\n",
    "\n",
    "total_samples = train_x.shape[0] + test_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Initialize model parameters ########################\n",
    "## For simplicity all time series TCNs have the same parameters, though it is relatively easy to change this\n",
    "tcn_kernel_size = 3\n",
    "tcn_layer_num = 7\n",
    "tcn_use_bias = True\n",
    "tcn_filter_num = 64\n",
    "tcn_kernel_initializer = 'random_normal'\n",
    "tcn_dropout_rate = 0.5 # This may be with the old keep_prob setting, we should also try the 1 - dropout_rate\n",
    "tcn_dropout_format = \"channel\"\n",
    "tcn_activation = 'relu'\n",
    "tcn_final_activation = 'linear'\n",
    "tcn_final_stack_activation = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Check for GPU\n",
    "\n",
    "## Make only given GPU visible   \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "mirrored_strategy = None\n",
    "\n",
    "print(\"GPUs Available: \", gpus)\n",
    "if len(gpus)==0:\n",
    "    device = \"CPU:0\"\n",
    "else:\n",
    "    print(\"Enter number of gpus to use:\")\n",
    "    gpu_num = input()\n",
    "    if len(gpu_num)!=0 and gpu_num.isdigit():\n",
    "        gpu_num = int(gpu_num)\n",
    "    if gpu_num==1:\n",
    "        print(\"Enter index of GPU to use:\")\n",
    "        gpu_idx = input()\n",
    "        if len(gpu_idx)!=0 and gpu_idx.isdigit():\n",
    "            gpu_idx = int(gpu_idx)\n",
    "        tf.config.experimental.set_visible_devices(gpus[gpu_idx], 'GPU')\n",
    "        device = \"GPU:0\"\n",
    "    else:\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy(devices=[F\"GPU:{i}\" for i in range(gpu_num)])\n",
    "        device = \" \".join([F\"GPU:{i}\" for i in range(gpu_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set evaluation seed to affect dropout random execution\n",
    "print(\"Enter a seed for the evaluation:\")\n",
    "seed = input()\n",
    "if len(seed)!=0 and seed.isdigit():\n",
    "    seed = int(seed)\n",
    "else:\n",
    "    seed = 192\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up test model\n",
    "## From all the test samples keep individual, non overlapping days\n",
    "unshuffled_train_days = unshuffled_data_windows[0::forecast_horizon]\n",
    "test_x_days = test_x[0::forecast_horizon,:]\n",
    "true_y = np.transpose(test_y[0::forecast_horizon,:],(0,2,1)).reshape((-1,len(targets)))\n",
    "\n",
    "expl_train_data_y = np.array(do_and_date['DO_mg'])[window_length:unshuffled_train_days.shape[0]*forecast_horizon+window_length]\n",
    "expl_test_data_y = np.array(do_and_date_test['DO_mg'])[:true_y.shape[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dropout = 0.85\n",
    "\n",
    "with tf.device(device):\n",
    "    test_model = MTCNAModel(tcn_layer_num,tcn_kernel_size,tcn_filter_num,window_length,forecast_horizon,len(targets), tcn_use_bias, tcn_kernel_initializer, test_dropout, tcn_dropout_format, tcn_activation, tcn_final_activation, tcn_final_stack_activation)\n",
    "_ = test_model(train_x[0:1])\n",
    "\n",
    "\n",
    "best_weight_name = \"f08332bc-d654-4219-a7c1-e0e6854fb2b5-weights.95-0.0070.h5\"\n",
    "\n",
    "## Generate predictions for test set using best weight (first in list)\n",
    "## Reset training fase to disable dropout \n",
    "tf.keras.backend.set_learning_phase(0)\n",
    "test_model.load_weights(\"SecondStageWeights-WaterQ/\"+best_weight_name)\n",
    "\n",
    "best_pred = np.asarray(test_model(test_x_days)[0]).reshape((len(targets),-1)).T\n",
    "if scale_output and \"Normalizer\" in str(out_preprocessor.__class__):\n",
    "    best_pred *= (out_norm)\n",
    "else:\n",
    "    best_pred = out_preprocessor.inverse_transform(best_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "weight_names = listdir(\"SecondStageWeights-WaterQ/\")\n",
    "print(weight_names)\n",
    "dropout_runs_per_weight = 20\n",
    "\n",
    "metrics_number = 6\n",
    "samples_per_prediction = dropout_runs_per_weight*len(weight_names)\n",
    "\n",
    "## Enable dropout\n",
    "tf.keras.backend.set_learning_phase(1)\n",
    "\n",
    "dl_errors  = np.zeros((samples_per_prediction,test_x_days.shape[0]*forecast_horizon,len(targets)))\n",
    "dl_predictions = np.zeros((samples_per_prediction,test_x_days.shape[0]*forecast_horizon,len(targets)))\n",
    "dl_metrics = np.zeros((samples_per_prediction,metrics_number,len(targets)))\n",
    "\n",
    "ml_train = np.zeros((samples_per_prediction,unshuffled_train_days.shape[0]*forecast_horizon,len(targets)))\n",
    "\n",
    "for i in tqdm(range(len(weight_names))):\n",
    "    test_model.load_weights(\"SecondStageWeights-WaterQ/\"+weight_names[i])\n",
    "    print(weight_names[i])\n",
    "    for j in range(dropout_runs_per_weight):\n",
    "        print(j)\n",
    "        ## Get DL test set predictions and metrics\n",
    "        cur_pred = np.asarray(test_model(test_x_days)[0]).reshape((len(targets),-1)).T\n",
    "        if scale_output and \"Normalizer\" in str(out_preprocessor.__class__):\n",
    "            cur_pred *= (out_norm)\n",
    "        else:\n",
    "            cur_pred = out_preprocessor.inverse_transform(cur_pred)\n",
    "        dl_predictions[i*dropout_runs_per_weight+j,:] = cur_pred\n",
    "        dl_errors[i*dropout_runs_per_weight+j,:] = cur_pred - true_y\n",
    "        for t in range(len(targets)):\n",
    "            dl_metrics[i*dropout_runs_per_weight+j,:,t] = np.asarray(get_metrics(true_y[:,t],cur_pred[:,t],print_metrics=False))\n",
    "        ## Get train set for explainable ML\n",
    "        cur_ml_train = np.asarray(test_model(unshuffled_train_days)[0]).reshape((len(targets),-1)).T\n",
    "        if scale_output and \"Normalizer\" in str(out_preprocessor.__class__):\n",
    "            cur_ml_train *= (out_norm)\n",
    "        else:\n",
    "            cur_ml_train = out_preprocessor.inverse_transform(cur_ml_train)\n",
    "        ml_train[i*dropout_runs_per_weight+j,:] = cur_ml_train\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.set_printoptions(linewidth=100)\n",
    "sns.set()\n",
    "for var_idx in range(len(targets)):\n",
    "    print(targets[var_idx])\n",
    "    present_mean_metrics(dl_metrics[...,var_idx])\n",
    "    plt.hist(dl_errors[...,var_idx].flatten(),alpha=0.5)\n",
    "    plt.hist((dl_predictions[...,var_idx]-np.median(dl_predictions[...,var_idx],axis=0)).flatten(),alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean, dl_lower_bound, dl_upper_bound = get_confidence_interval_series(dl_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preceding_points = 192\n",
    "from_day = 20\n",
    "to_day = 21\n",
    "\n",
    "\n",
    "d0 = holdout_set_start_date.to_pydatetime()\n",
    "d1 = d0 + timedelta(days=from_day)\n",
    "\n",
    "pred_plot_range = range(preceding_points,preceding_points+(to_day-from_day)*forecast_horizon)\n",
    "pred_sp = from_day*forecast_horizon\n",
    "pred_ep = to_day*forecast_horizon\n",
    "\n",
    "for i in range(len(targets)):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    plt.plot(pred_plot_range,pred_mean[pred_sp:pred_ep,i],marker=\"o\",label=\"Prediction\")\n",
    "    plt.fill_between(pred_plot_range, dl_lower_bound[pred_sp:pred_ep,i], dl_upper_bound[pred_sp:pred_ep,i], alpha=0.3)\n",
    "    \n",
    "    if from_day==0:\n",
    "        plt.plot(pre_test_target[-preceding_points:,i],label=\"Pretest period\", marker=\"o\")\n",
    "    else:\n",
    "        plt.plot(true_y[pred_sp-preceding_points:pred_sp,i],label=\"Pretest period\", marker=\"o\")\n",
    "    plt.plot(pred_plot_range,true_y[from_day*forecast_horizon:to_day*forecast_horizon,i],marker=\"o\",label=\"True data\")\n",
    "\n",
    "    plt.grid(axis='x')\n",
    "    plt.legend()\n",
    "    plt.title(targets[i])\n",
    "    plt.xlabel(d1.strftime(\"%d/%m/%Y\"))\n",
    "    plt.xticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sp = pre_test_target.shape[0]\n",
    "ep = sp + true_y.shape[0]\n",
    "days_ahead=30\n",
    "\n",
    "for i in range(len(targets)):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    plt.plot(range(sp,ep),best_pred[:,i],label=\"Prediction\")\n",
    "\n",
    "    plt.plot(pre_test_target[:,i],label=\"Pretest period\")\n",
    "    plt.plot(range(sp,ep),true_y[:,i],label=\"True data\")\n",
    "\n",
    "    plt.xlim(left=sp, right=sp+days_ahead*forecast_horizon)\n",
    "    plt.title(targets[i])\n",
    "    plt.grid(axis='x')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Present attention graphs for specific prediction output\n",
    "\n",
    "input_variables = ['Temp_degC', 'EC_uScm', 'pH', 'Turbidity_NTU', 'Chloraphylla_ugL']\n",
    "\n",
    "var_of_interest = \"Temp_degC\"\n",
    "\n",
    "var_idx = input_variables.index(var_of_interest)\n",
    "\n",
    "test_idx = 20\n",
    "\n",
    "## Reset training fase to disable dropout \n",
    "tf.keras.backend.set_learning_phase(0)\n",
    "test_model.load_weights(\"SecondStageWeights/\"+best_weight_name)\n",
    "\n",
    "\n",
    "o, dist = test_model(test_x_days[test_idx:test_idx+1])\n",
    "\n",
    "o = np.asarray(o).reshape((len(targets),-1)).T\n",
    "if scale_output and \"Normalizer\" in str(out_preprocessor.__class__):\n",
    "    o *= (out_norm)\n",
    "else:\n",
    "    o = out_preprocessor.inverse_transform(o)\n",
    "inp = preprocessor.inverse_transform(test_x_days[test_idx])[:,var_idx]\n",
    "\n",
    "prediction= o[:,var_idx]\n",
    "true_out = true_y[test_idx*forecast_horizon:(test_idx+1)*(forecast_horizon),var_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(figsize=(20,10))\n",
    "plt.plot(inp)\n",
    "plt.plot(np.arange(window_length,window_length+forecast_horizon),prediction,marker=\"o\",label=\"Prediction\")\n",
    "plt.plot(np.arange(window_length,window_length+forecast_horizon),true_out,marker=\"o\",label=\"Ground truth\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get value dense layer\n",
    "for w in test_model.weights:\n",
    "    if w.name.endswith(\"sep_dense_value_weights:0\"):\n",
    "        weights = np.abs(w.numpy())[var_idx]\n",
    "        #weights = w.numpy()[var_idx]\n",
    "        break\n",
    "\n",
    "dist_var = dist.numpy()[0,var_idx,...]\n",
    "full_dist = np.matmul(dist_var,weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "def infl_to_out_elem(out_elem):\n",
    "    elem_dist = full_dist[out_elem:out_elem+1,:]\n",
    "    prep = preprocessing.MinMaxScaler()\n",
    "    prep.fit(elem_dist.T)\n",
    "    elem_dist = prep.transform(elem_dist.T)\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    sns.heatmap(elem_dist.T, cmap=\"Blues\", cbar=True, yticklabels=False, xticklabels=10)\n",
    "    ax2 = plt.twinx()\n",
    "    ax2.plot(range(window_length,window_length+forecast_horizon),true_out,label=\"True data\",marker=\"o\")\n",
    "    ax2.plot(range(window_length,window_length+forecast_horizon),prediction,label=\"Prediction\",marker=\"o\")\n",
    "    plt.plot([window_length+out_elem], [prediction[out_elem]], marker='o', label= \"Step \"+str(out_elem+1), markersize=8, color=\"black\")\n",
    "    sns.lineplot(x=np.arange(0,window_length),y=inp, ax=ax2)\n",
    "    ax.axis('tight')\n",
    "    ax2.legend(fontsize=20)\n",
    "    ax2.set_ylabel(\"°C\")\n",
    "    plt.show()\n",
    "    #plt.savefig(\"%s-%02d.png\"%(var_of_interest,out_elem))\n",
    "    #plt.close(fig)\n",
    "#infl_to_out_elem(22)\n",
    "interact(infl_to_out_elem, out_elem=(0,forecast_horizon-1,1))\n",
    "# for i in range(forecast_horizon):\n",
    "#     infl_to_out_elem(i)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare dataset for explainable methods from output of stage 1, do_mg values and timestamp\n",
    "## Extract timestamp information to periodic numbers\n",
    "date_linear_df_train = pd.DataFrame()\n",
    "date_cat_df_train = pd.DataFrame()\n",
    "date_linear_df_test = pd.DataFrame()\n",
    "date_cat_df_test = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "date_linear_df_train['hour_sin'] = np.sin(do_and_date['DateObj'].dt.hour*(2.*np.pi/24))\n",
    "date_linear_df_train['hour_cos'] = np.cos(do_and_date['DateObj'].dt.hour*(2.*np.pi/24))\n",
    "\n",
    "date_linear_df_train['dayofyear_sin'] = np.sin((do_and_date['DateObj'].dt.dayofyear-1)*(2.*np.pi/365))\n",
    "date_linear_df_train['dayofyear_cos'] = np.cos((do_and_date['DateObj'].dt.dayofyear-1)*(2.*np.pi/365))\n",
    "\n",
    "date_linear_df_train['month_sin'] = np.sin((do_and_date['DateObj'].dt.month-1)*(2.*np.pi/12))\n",
    "date_linear_df_train['month_cos'] = np.cos((do_and_date['DateObj'].dt.month-1)*(2.*np.pi/12))\n",
    "\n",
    "date_linear_df_train['quarter_sin'] = np.sin((do_and_date['DateObj'].dt.quarter-1)*(2.*np.pi/4))\n",
    "date_linear_df_train['quarter_cos'] = np.cos((do_and_date['DateObj'].dt.quarter-1)*(2.*np.pi/4))\n",
    "\n",
    "date_linear_df_train['dayofweek_sin'] = np.sin((do_and_date['DateObj'].dt.dayofweek)*(2.*np.pi/7))\n",
    "date_linear_df_train['dayofweek_cos'] = np.cos((do_and_date['DateObj'].dt.dayofweek)*(2.*np.pi/7))\n",
    "\n",
    "date_linear_df_train['dayofmonth_sin'] = np.sin((do_and_date['DateObj'].dt.day-1)*(2.*np.pi/31))\n",
    "date_linear_df_train['dayofmonth_cos'] = np.cos((do_and_date['DateObj'].dt.day-1)*(2.*np.pi/31))\n",
    "\n",
    "expl_train_linear_date_data = np.array(date_linear_df_train)[window_length:expl_train_data_y.shape[0]+window_length]\n",
    "\n",
    "\n",
    "\n",
    "date_cat_df_train['hour'] = do_and_date['DateObj'].dt.hour\n",
    "date_cat_df_train['dayofyear'] = do_and_date['DateObj'].dt.dayofyear-1\n",
    "date_cat_df_train['month'] = do_and_date['DateObj'].dt.month-1\n",
    "date_cat_df_train['quarter'] = do_and_date['DateObj'].dt.quarter-1\n",
    "date_cat_df_train['dayofweek'] = do_and_date['DateObj'].dt.dayofweek\n",
    "date_cat_df_train['dayofmonth'] = do_and_date['DateObj'].dt.day-1\n",
    "\n",
    "expl_train_cat_date_data = np.array(date_cat_df_train)[window_length:expl_train_data_y.shape[0]+window_length]\n",
    "\n",
    "## Same process for test data\n",
    "\n",
    "date_linear_df_test['hour_sin'] = np.sin(do_and_date_test['DateObj'].dt.hour*(2.*np.pi/24))\n",
    "date_linear_df_test['hour_cos'] = np.cos(do_and_date_test['DateObj'].dt.hour*(2.*np.pi/24))\n",
    "\n",
    "date_linear_df_test['dayofyear_sin'] = np.sin((do_and_date_test['DateObj'].dt.dayofyear-1)*(2.*np.pi/365))\n",
    "date_linear_df_test['dayofyear_cos'] = np.cos((do_and_date_test['DateObj'].dt.dayofyear-1)*(2.*np.pi/365))\n",
    "\n",
    "date_linear_df_test['month_sin'] = np.sin((do_and_date_test['DateObj'].dt.month-1)*(2.*np.pi/12))\n",
    "date_linear_df_test['month_cos'] = np.cos((do_and_date_test['DateObj'].dt.month-1)*(2.*np.pi/12))\n",
    "\n",
    "date_linear_df_test['quarter_sin'] = np.sin((do_and_date_test['DateObj'].dt.quarter-1)*(2.*np.pi/4))\n",
    "date_linear_df_test['quarter_cos'] = np.cos((do_and_date_test['DateObj'].dt.quarter-1)*(2.*np.pi/4))\n",
    "\n",
    "date_linear_df_test['dayofweek_sin'] = np.sin((do_and_date_test['DateObj'].dt.dayofweek)*(2.*np.pi/7))\n",
    "date_linear_df_test['dayofweek_cos'] = np.cos((do_and_date_test['DateObj'].dt.dayofweek)*(2.*np.pi/7))\n",
    "\n",
    "date_linear_df_test['dayofmonth_sin'] = np.sin((do_and_date_test['DateObj'].dt.day-1)*(2.*np.pi/31))\n",
    "date_linear_df_test['dayofmonth_cos'] = np.cos((do_and_date_test['DateObj'].dt.day-1)*(2.*np.pi/31))\n",
    "\n",
    "expl_test_linear_date_data = np.array(date_linear_df_test)[:true_y.shape[0]]\n",
    "\n",
    "\n",
    "\n",
    "date_cat_df_test['hour'] = do_and_date_test['DateObj'].dt.hour\n",
    "date_cat_df_test['dayofyear'] = do_and_date_test['DateObj'].dt.dayofyear -1\n",
    "date_cat_df_test['month'] = do_and_date_test['DateObj'].dt.month-1\n",
    "date_cat_df_test['quarter'] = do_and_date_test['DateObj'].dt.quarter-1\n",
    "date_cat_df_test['dayofweek'] = do_and_date_test['DateObj'].dt.dayofweek\n",
    "date_cat_df_test['dayofmonth'] = do_and_date_test['DateObj'].dt.day-1\n",
    "\n",
    "expl_test_cat_date_data = np.array(date_cat_df_test)[:true_y.shape[0]]\n",
    "                                                          \n",
    "# Add information of DO_mg value at last known step\n",
    "expl_train_domg_last_step = np.zeros((expl_train_data_y.shape[0],1))\n",
    "for i in range(0,expl_train_data_y.shape[0],forecast_horizon):\n",
    "    expl_train_domg_last_step[i:i+forecast_horizon] = do_and_date['DO_mg'].iloc[window_length-1+i]\n",
    "    \n",
    "expl_train_domg_last_mean = np.zeros((expl_train_data_y.shape[0],1))\n",
    "for i in range(0,expl_train_data_y.shape[0],forecast_horizon):\n",
    "    expl_train_domg_last_mean[i:i+forecast_horizon] = np.mean(do_and_date['DO_mg'].iloc[window_length-forecast_horizon+i:window_length+i])\n",
    "\n",
    "expl_test_domg_last_step = np.zeros((expl_test_data_y.shape[0],1))\n",
    "expl_test_domg_last_step[0:forecast_horizon] = do_and_date['DO_mg'].iloc[-1]\n",
    "for i in range(forecast_horizon,expl_test_data_y.shape[0],forecast_horizon):\n",
    "    expl_test_domg_last_step[i:i+forecast_horizon] = do_and_date_test['DO_mg'].iloc[i-1]\n",
    "\n",
    "expl_test_domg_last_mean = np.zeros((expl_test_data_y.shape[0],1))\n",
    "expl_test_domg_last_mean[0:forecast_horizon] = np.mean(do_and_date['DO_mg'].iloc[-forecast_horizon:])\n",
    "for i in range(forecast_horizon,expl_test_data_y.shape[0],forecast_horizon):\n",
    "    expl_test_domg_last_mean[i:i+forecast_horizon] = np.mean(do_and_date_test['DO_mg'].iloc[i-forecast_horizon:i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression\n",
    "lr_dtypes = [\"linear\",\"categorical\"]\n",
    "\n",
    "lr_dtype_idx = 0\n",
    "\n",
    "lr_dtype = lr_dtypes[lr_dtype_idx]\n",
    "\n",
    "print(\"Approach with %s data.\"%lr_dtype)\n",
    "\n",
    "\n",
    "\n",
    "feature_names=input_variables+list(date_linear_df_train.columns) +[\"last_DO_mg_step\"]+[\"last_DO_mg_mean\"]\n",
    "print(len(feature_names))\n",
    "print(feature_names)\n",
    "linreg= LinearRegression()\n",
    "linear_reg_predictions = np.zeros((ml_train.shape[0],expl_test_data_y.shape[0]))\n",
    "linear_reg_metrics = np.zeros((ml_train.shape[0],metrics_number))\n",
    "## Linear datasets\n",
    "for i in range(ml_train.shape[0]):\n",
    "    \n",
    "    expl_train_data_x = ml_train[i,...]\n",
    "    expl_test_data_x = dl_predictions[i,...]\n",
    "    \n",
    "    if lr_dtype==\"linear\":    \n",
    "        expl_linear_train_data_x = np.hstack((expl_train_data_x,expl_train_linear_date_data))\n",
    "        expl_linear_train_data_x = np.hstack((expl_linear_train_data_x,expl_train_domg_last_step))\n",
    "        expl_linear_train_data_x = np.hstack((expl_linear_train_data_x,expl_train_domg_last_mean))\n",
    "\n",
    "        expl_linear_test_data_x = np.hstack((expl_test_data_x,expl_test_linear_date_data))\n",
    "        expl_linear_test_data_x = np.hstack((expl_linear_test_data_x,expl_test_domg_last_step))\n",
    "        expl_linear_test_data_x = np.hstack((expl_linear_test_data_x,expl_test_domg_last_mean))\n",
    "    else:        \n",
    "        expl_cat_train_data_x = np.hstack((expl_train_data_x,expl_train_cat_date_data))\n",
    "        expl_cat_train_data_x = np.hstack((expl_cat_train_data_x,expl_train_domg_last_step))\n",
    "        expl_cat_train_data_x = np.hstack((expl_cat_train_data_x,expl_train_domg_last_mean))\n",
    "\n",
    "        expl_cat_test_data_x = np.hstack((expl_test_data_x,expl_test_cat_date_data))\n",
    "        expl_cat_test_data_x = np.hstack((expl_cat_test_data_x,expl_test_domg_last_step))\n",
    "        expl_cat_test_data_x = np.hstack((expl_cat_test_data_x,expl_test_domg_last_mean))\n",
    "\n",
    "    if lr_dtype==\"linear\":\n",
    "        expl_train_x = expl_linear_train_data_x.copy()\n",
    "        expl_test_x = expl_linear_test_data_x.copy()\n",
    "    else:\n",
    "        expl_train_x = expl_cat_train_data_x.copy()\n",
    "        expl_test_x = expl_cat_test_data_x.copy()\n",
    "    \n",
    "    \n",
    "    linreg.fit(expl_train_x,expl_train_data_y)\n",
    "    cur_pred = linreg.predict(expl_test_x)\n",
    "    linear_reg_predictions[i,...] = cur_pred\n",
    "    linear_reg_metrics[i,...] = np.asarray(get_metrics(expl_test_data_y,cur_pred))\n",
    "\n",
    "lrmean, lrlb, lrup = get_confidence_interval_series(linear_reg_predictions)\n",
    "\n",
    "present_mean_metrics(linear_reg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = holdout_set_start_date.to_pydatetime()\n",
    "preceding_points = 24\n",
    "from_day =20\n",
    "to_day = 21\n",
    "d1 = d0 + timedelta(days=from_day)\n",
    "\n",
    "pred_plot_range = range(preceding_points,preceding_points+(to_day-from_day)*forecast_horizon)\n",
    "pred_sp = from_day*forecast_horizon\n",
    "pred_ep = to_day*forecast_horizon\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.plot(pred_plot_range,lrmean[pred_sp:pred_ep],marker=\"o\",label=\"Prediction\")\n",
    "plt.fill_between(pred_plot_range, lrlb[pred_sp:pred_ep], lrup[pred_sp:pred_ep], alpha=0.3)\n",
    "\n",
    "if from_day==0:\n",
    "    plt.plot(np.array(do_and_date['DO_mg'])[-preceding_points:],label=\"Pretest period\", marker=\"o\")\n",
    "else:\n",
    "    plt.plot(expl_test_data_y[pred_sp-preceding_points:pred_sp],label=\"Pretest period\", marker=\"o\")\n",
    "plt.plot(pred_plot_range,expl_test_data_y[from_day*forecast_horizon:to_day*forecast_horizon],marker=\"o\",label=\"True data\")\n",
    "\n",
    "plt.grid(axis='x')\n",
    "plt.ylim(top=7.65)\n",
    "plt.legend(fontsize=35)\n",
    "plt.tick_params(axis=\"y\", labelsize=35)\n",
    "plt.xlabel(d1.strftime(\"%d/%m/%Y\"),fontsize=35)\n",
    "plt.ylabel(\"mg/L\",fontsize=35)\n",
    "plt.xticks([])\n",
    "plt.savefig(\"linear_reg.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eli5.sklearn.explain_linear_regressor_weights(linreg,feature_names=feature_names,top=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision tree regressor\n",
    "dt_dtypes = [\"linear\",\"categorical\"]\n",
    "\n",
    "dt_dtype_idx = 0\n",
    "\n",
    "dt_dtype = dt_dtypes[dt_dtype_idx]\n",
    "\n",
    "print(\"Approach with %s data.\"%dt_dtype)\n",
    "\n",
    "if dt_dtype==\"linear\":\n",
    "    feature_names=input_variables+list(date_linear_df_train.columns) +[\"last_DO_mg_step\"]+[\"last_DO_mg_mean\"]\n",
    "else:\n",
    "    feature_names=input_variables+list(date_cat_df_train.columns) +[\"last_DO_mg_step\"]+[\"last_DO_mg_mean\"]\n",
    "    \n",
    "dec_tree = DecisionTreeRegressor(max_depth=6)\n",
    "dec_tree_predictions = np.zeros((ml_train.shape[0],expl_test_data_y.shape[0]))\n",
    "dec_tree_metrics = np.zeros((ml_train.shape[0],metrics_number))\n",
    "## Linear datasets\n",
    "for i in range(ml_train.shape[0]):\n",
    "    expl_train_data_x = ml_train[i,...]\n",
    "    expl_test_data_x = dl_predictions[i,...]\n",
    "    \n",
    "    if dt_dtype==\"linear\":    \n",
    "        expl_linear_train_data_x = np.hstack((expl_train_data_x,expl_train_linear_date_data))\n",
    "        expl_linear_train_data_x = np.hstack((expl_linear_train_data_x,expl_train_domg_last_step))\n",
    "        expl_linear_train_data_x = np.hstack((expl_linear_train_data_x,expl_train_domg_last_mean))\n",
    "\n",
    "        expl_linear_test_data_x = np.hstack((expl_test_data_x,expl_test_linear_date_data))\n",
    "        expl_linear_test_data_x = np.hstack((expl_linear_test_data_x,expl_test_domg_last_step))\n",
    "        expl_linear_test_data_x = np.hstack((expl_linear_test_data_x,expl_test_domg_last_mean))\n",
    "    else:        \n",
    "        expl_cat_train_data_x = np.hstack((expl_train_data_x,expl_train_cat_date_data))\n",
    "        expl_cat_train_data_x = np.hstack((expl_cat_train_data_x,expl_train_domg_last_step))\n",
    "        expl_cat_train_data_x = np.hstack((expl_cat_train_data_x,expl_train_domg_last_mean))\n",
    "\n",
    "        expl_cat_test_data_x = np.hstack((expl_test_data_x,expl_test_cat_date_data))\n",
    "        expl_cat_test_data_x = np.hstack((expl_cat_test_data_x,expl_test_domg_last_step))\n",
    "        expl_cat_test_data_x = np.hstack((expl_cat_test_data_x,expl_test_domg_last_mean))\n",
    "    if dt_dtype==\"linear\":\n",
    "        expl_train_x = expl_linear_train_data_x.copy()\n",
    "        expl_test_x = expl_linear_test_data_x.copy()\n",
    "    else:\n",
    "        expl_train_x = expl_cat_train_data_x.copy()\n",
    "        expl_test_x = expl_cat_test_data_x.copy()\n",
    "    \n",
    "    \n",
    "    dec_tree.fit(expl_train_x,expl_train_data_y)\n",
    "    cur_pred = dec_tree.predict(expl_test_x)\n",
    "    dec_tree_predictions[i,...] = cur_pred\n",
    "    dec_tree_metrics[i,...] = np.asarray(get_metrics(expl_test_data_y,cur_pred))\n",
    "\n",
    "dtmean, dtlb, dtup = get_confidence_interval_series(dec_tree_predictions)\n",
    "\n",
    "present_mean_metrics(dec_tree_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_plot_range = range(preceding_points,preceding_points+(to_day-from_day)*forecast_horizon)\n",
    "pred_sp = from_day*forecast_horizon\n",
    "pred_ep = to_day*forecast_horizon\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.plot(pred_plot_range,dtmean[pred_sp:pred_ep],marker=\"o\",label=\"Prediction\")\n",
    "plt.fill_between(pred_plot_range, dtlb[pred_sp:pred_ep], dtup[pred_sp:pred_ep], alpha=0.3)\n",
    "\n",
    "if from_day==0:\n",
    "    plt.plot(pre_test_target[-preceding_points:],label=\"Pretest period\", marker=\"o\")\n",
    "else:\n",
    "    plt.plot(expl_test_data_y[pred_sp-preceding_points:pred_sp],label=\"Pretest period\", marker=\"o\")\n",
    "\n",
    "plt.plot(pred_plot_range,expl_test_data_y[from_day*forecast_horizon:to_day*forecast_horizon],marker=\"o\",label=\"True data\")\n",
    "plt.grid(axis='x')\n",
    "plt.legend(fontsize=35)\n",
    "plt.tick_params(axis=\"y\", labelsize=35)\n",
    "plt.xlabel(d1.strftime(\"%d/%m/%Y\"),fontsize=35)\n",
    "plt.ylabel(\"mg/L\",fontsize=35)\n",
    "plt.ylim(top=7.65)\n",
    "plt.xticks([])\n",
    "plt.savefig(\"dec_tree.png\")\n",
    "plt.show()\n",
    "\n",
    "eli5.sklearn.explain_decision_tree(dec_tree,feature_names=feature_names)\n",
    "#dot_data = export_graphviz(dec_tree, max_depth=6,feature_names=feature_names)\n",
    "#graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "#Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Simple linear GAM\n",
    "\n",
    "\n",
    "gam_dtypes = [\"linear\",\"categorical\"]\n",
    "\n",
    "gam_dtype_idx = 0\n",
    "\n",
    "gam_dtype = gam_dtypes[gam_dtype_idx]\n",
    "print(\"Approach with %s data.\"%gam_dtype)\n",
    "\n",
    "if gam_dtype==\"linear\":\n",
    "    feature_names=input_variables+list(date_linear_df_train.columns) +[\"last_DO_mg_step\"]+[\"last_DO_mg_mean\"]\n",
    "else:\n",
    "    feature_names=input_variables+list(date_cat_df_train.columns) +[\"last_DO_mg_step\"]+[\"last_DO_mg_mean\"]\n",
    "    \n",
    "    \n",
    "##Gam hyperparameters, as a result of Grid search on training data\n",
    "lam = 0.004\n",
    "spline_order=5\n",
    "n_splines=7\n",
    "\n",
    "\n",
    "term_splines = [s(i,spline_order=spline_order, n_splines=n_splines,lam=lam) for i in range(len(feature_names))]\n",
    "termlist = terms.TermList(*term_splines)\n",
    "\n",
    "gam_predictions = np.zeros((ml_train.shape[0],expl_test_data_y.shape[0]))\n",
    "gam_metrics = np.zeros((ml_train.shape[0],metrics_number))\n",
    "for i in tqdm(range(ml_train.shape[0])): \n",
    "    expl_train_data_x = ml_train[i,...]\n",
    "    expl_test_data_x = dl_predictions[i,...]\n",
    "    \n",
    "    if gam_dtype==\"linear\":    \n",
    "        expl_linear_train_data_x = np.hstack((expl_train_data_x,expl_train_linear_date_data))\n",
    "        expl_linear_train_data_x = np.hstack((expl_linear_train_data_x,expl_train_domg_last_step))\n",
    "        expl_linear_train_data_x = np.hstack((expl_linear_train_data_x,expl_train_domg_last_mean))\n",
    "\n",
    "        expl_linear_test_data_x = np.hstack((expl_test_data_x,expl_test_linear_date_data))\n",
    "        expl_linear_test_data_x = np.hstack((expl_linear_test_data_x,expl_test_domg_last_step))\n",
    "        expl_linear_test_data_x = np.hstack((expl_linear_test_data_x,expl_test_domg_last_mean))\n",
    "    else:        \n",
    "        expl_cat_train_data_x = np.hstack((expl_train_data_x,expl_train_cat_date_data))\n",
    "        expl_cat_train_data_x = np.hstack((expl_cat_train_data_x,expl_train_domg_last_step))\n",
    "        expl_cat_train_data_x = np.hstack((expl_cat_train_data_x,expl_train_domg_last_mean))\n",
    "\n",
    "        expl_cat_test_data_x = np.hstack((expl_test_data_x,expl_test_cat_date_data))\n",
    "        expl_cat_test_data_x = np.hstack((expl_cat_test_data_x,expl_test_domg_last_step))\n",
    "        expl_cat_test_data_x = np.hstack((expl_cat_test_data_x,expl_test_domg_last_mean))\n",
    "        \n",
    "    if gam_dtype==\"linear\":\n",
    "        expl_train_x = expl_linear_train_data_x.copy()\n",
    "        expl_test_x = expl_linear_test_data_x.copy()\n",
    "    else:\n",
    "        expl_train_x = expl_cat_train_data_x.copy()\n",
    "        expl_test_x = expl_cat_test_data_x.copy()\n",
    "    \n",
    "    gam = LinearGAM(termlist,fit_intercept = True).fit(expl_train_x, expl_train_data_y)\n",
    "    cur_pred = gam.predict(expl_test_x)\n",
    "    gam_predictions[i,...] = cur_pred\n",
    "    gam_metrics[i,...] = np.asarray(get_metrics(expl_test_data_y,cur_pred)) \n",
    "\n",
    "gammean, gamlb, gamup = get_confidence_interval_series(gam_predictions)\n",
    "\n",
    "present_mean_metrics(gam_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preceding_points = 24\n",
    "\n",
    "pred_plot_range = range(preceding_points,preceding_points+(to_day-from_day)*forecast_horizon)\n",
    "pred_sp = from_day*forecast_horizon\n",
    "pred_ep = to_day*forecast_horizon\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.plot(pred_plot_range,gammean[pred_sp:pred_ep],label=\"Prediction\",marker=\"o\")\n",
    "\n",
    "plt.fill_between(pred_plot_range, gamlb[pred_sp:pred_ep], gamup[pred_sp:pred_ep], alpha=0.3)\n",
    "\n",
    "if from_day==0:\n",
    "    plt.plot(np.array(do_and_date['DO_mg'])[-preceding_points:],label=\"Pretest period\", marker=\"o\")\n",
    "else:\n",
    "    plt.plot(expl_test_data_y[pred_sp-preceding_points:pred_sp],label=\"Pretest period\", marker=\"o\")\n",
    "plt.plot(pred_plot_range,expl_test_data_y[from_day*forecast_horizon:to_day*forecast_horizon],label=\"True data\",marker=\"o\")\n",
    "plt.grid(axis='x')\n",
    "plt.legend(fontsize=35)\n",
    "plt.tick_params(axis=\"y\", labelsize=35)\n",
    "plt.xlabel(d1.strftime(\"%d/%m/%Y\"),fontsize=35)\n",
    "plt.ylabel(\"mg/L\",fontsize=35)\n",
    "plt.ylim(top=7.65)\n",
    "plt.xticks([])\n",
    "#plt.savefig(\"gam.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_names)\n",
    "term = 'Temp_degC'\n",
    "term_idx = feature_names.index(term)\n",
    "XX = gam.generate_X_grid(term=term_idx)\n",
    "pdep, confi = gam.partial_dependence(term=term_idx, X=XX, width=0.95)\n",
    "plt.figure(figsize=(15,10))\n",
    "term = gam.terms[term_idx]\n",
    "plt.plot(XX[:, term.feature], pdep)\n",
    "plt.plot(XX[:, term.feature], confi, c='r', ls='--')\n",
    "plt.title(feature_names[term_idx])\n",
    "# plt.savefig(\"gampdplot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
